{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e54dcb",
   "metadata": {},
   "source": [
    "# RAG Document Processing Pipeline\n",
    "\n",
    "This notebook processes educational documents in batch from a local folder and uploads them to ChromaDB for use by the backend system.\n",
    "\n",
    "## Features\n",
    "- Processes PDF, DOCX, TXT, and Markdown files\n",
    "- Automatic metadata extraction from filenames\n",
    "- Document chunking with overlap\n",
    "- Embedding generation using sentence transformers\n",
    "- Batch upload to ChromaDB with progress tracking\n",
    "- Error handling and logging\n",
    "\n",
    "## Usage\n",
    "1. Place your documents in the `./documents/` folder\n",
    "2. Configure the settings below\n",
    "3. Run all cells to process and upload documents\n",
    "4. Verify results using the testing section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21664ccd",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Dependencies\n",
    "\n",
    "**Important:** This notebook requires several Python packages. If you get import errors, run the installation cell below first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a407ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Installing required packages...\n",
      "This may take a few minutes for first-time installation.\n",
      "--------------------------------------------------\n",
      "üì¶ Installing chromadb...\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-1.1.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.2 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pydantic>=1.9 (from chromadb)\n",
      "  Using cached pydantic-2.12.0-py3-none-any.whl.metadata (83 kB)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (2.3.3)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (4.15.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.23.1-cp313-cp313-macosx_13_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (6.5.2)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Using cached grpcio-1.75.1-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (5.0.0)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (6.0.3)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from chromadb) (3.11.3)\n",
      "Collecting httpx>=0.27.0 (from chromadb)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting jsonschema>=4.19.0 (from chromadb)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting requests<3.0,>=2.7 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2025.10.5)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Collecting anyio (from httpx>=0.27.0->chromadb)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.41.1-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.1)\n",
      "Requirement already satisfied: sympy in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Collecting pydantic-core==2.41.1 (from pydantic>=1.9->chromadb)\n",
      "  Using cached pydantic_core-2.41.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Collecting huggingface-hub<2.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached watchfiles-1.1.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Using cached chromadb-1.1.1-cp39-abi3-macosx_11_0_arm64.whl (18.3 MB)\n",
      "Using cached posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Using cached grpcio-1.75.1-cp313-cp313-macosx_11_0_universal2.whl (11.5 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "Using cached google_auth-2.41.1-py2.py3-none-any.whl (221 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached onnxruntime-1.23.1-cp313-cp313-macosx_13_0_arm64.whl (17.2 MB)\n",
      "Using cached opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached pydantic-2.12.0-py3-none-any.whl (459 kB)\n",
      "Using cached pydantic_core-2.41.1-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "Using cached typer-0.19.2-py3-none-any.whl (46 kB)\n",
      "Using cached watchfiles-1.1.0-cp313-cp313-macosx_11_0_arm64.whl (393 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Installing collected packages: rsa, requests, referencing, pydantic-core, pyasn1-modules, opentelemetry-proto, markdown-it-py, importlib-metadata, httpcore, grpcio, googleapis-common-protos, coloredlogs, build, anyio, watchfiles, rich, requests-oauthlib, pydantic, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, jsonschema-specifications, huggingface-hub, httpx, google-auth, typer, tokenizers, opentelemetry-semantic-conventions, kubernetes, jsonschema, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34/34\u001b[0m [chromadb]/34\u001b[0m [chromadb]s]antic-conventions]\n",
      "\u001b[1A\u001b[2KSuccessfully installed anyio-4.11.0 build-1.3.0 chromadb-1.1.1 coloredlogs-15.0.1 google-auth-2.41.1 googleapis-common-protos-1.70.0 grpcio-1.75.1 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.35.3 importlib-metadata-8.7.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 kubernetes-34.1.0 markdown-it-py-4.0.0 onnxruntime-1.23.1 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 posthog-5.4.0 pyasn1-modules-0.4.2 pydantic-2.12.0 pydantic-core-2.41.1 referencing-0.36.2 requests-2.32.5 requests-oauthlib-2.0.0 rich-14.1.0 rsa-4.9.1 tokenizers-0.22.1 typer-0.19.2 watchfiles-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully installed chromadb\n",
      "üì¶ Installing sentence-transformers...\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Using cached sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Using cached transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Installing collected packages: jinja2, torch, scikit-learn, transformers, sentence-transformers\n",
      "\u001b[?25lCollecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Using cached sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Using cached transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Installing collected packages: jinja2, torch, scikit-learn, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5/5\u001b[0m [sentence-transformers]sformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jinja2-3.1.6 scikit-learn-1.7.2 sentence-transformers-5.1.1 torch-2.8.0 transformers-4.57.0\n",
      "‚úÖ Successfully installed sentence-transformers\n",
      "üì¶ Installing openai...\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5/5\u001b[0m [sentence-transformers]sformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jinja2-3.1.6 scikit-learn-1.7.2 sentence-transformers-5.1.1 torch-2.8.0 transformers-4.57.0\n",
      "‚úÖ Successfully installed sentence-transformers\n",
      "üì¶ Installing openai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-2.2.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from openai) (2.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Using cached openai-2.2.0-py3-none-any.whl (998 kB)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-2.2.0\n",
      "‚úÖ Successfully installed openai\n",
      "‚úÖ PyPDF2 already installed\n",
      "üì¶ Installing python-docx...\n",
      "Successfully installed openai-2.2.0\n",
      "‚úÖ Successfully installed openai\n",
      "‚úÖ PyPDF2 already installed\n",
      "üì¶ Installing python-docx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Using cached python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from python-docx) (6.0.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from python-docx) (4.15.0)\n",
      "Using cached python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully installed python-docx\n",
      "‚úÖ tqdm already installed\n",
      "‚úÖ numpy already installed\n",
      "üì¶ Installing pydantic-settings...\n",
      "Collecting pydantic-settings\n",
      "  Using cached pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic-settings) (2.12.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic-settings) (1.1.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic-settings) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic>=2.7.0->pydantic-settings) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic>=2.7.0->pydantic-settings) (2.41.1)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic>=2.7.0->pydantic-settings) (4.15.0)\n",
      "Using cached pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "Installing collected packages: pydantic-settings\n",
      "Successfully installed pydantic-settings-2.11.0\n",
      "‚úÖ Successfully installed pydantic-settings\n",
      "--------------------------------------------------\n",
      "‚úÖ Dependency installation complete!\n",
      "üí° You can now run the rest of the notebook cells.\n",
      "Collecting pydantic-settings\n",
      "  Using cached pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic-settings) (2.12.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic-settings) (1.1.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic-settings) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic>=2.7.0->pydantic-settings) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic>=2.7.0->pydantic-settings) (2.41.1)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/zsolt/Downloads/edu_platform/.venv/lib/python3.13/site-packages (from pydantic>=2.7.0->pydantic-settings) (4.15.0)\n",
      "Using cached pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "Installing collected packages: pydantic-settings\n",
      "Successfully installed pydantic-settings-2.11.0\n",
      "‚úÖ Successfully installed pydantic-settings\n",
      "--------------------------------------------------\n",
      "‚úÖ Dependency installation complete!\n",
      "üí° You can now run the rest of the notebook cells.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "# Run this cell if you get ModuleNotFoundError or import errors\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ Successfully installed {package}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_package(package_name, import_name=None):\n",
    "    \"\"\"Check if a package is available for import.\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name.replace(\"-\", \"_\")\n",
    "    \n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Required packages with their import names\n",
    "required_packages = [\n",
    "    (\"chromadb\", \"chromadb\"),\n",
    "    (\"sentence-transformers\", \"sentence_transformers\"), \n",
    "    (\"openai\", \"openai\"),\n",
    "    (\"PyPDF2\", \"PyPDF2\"),\n",
    "    (\"python-docx\", \"docx\"),\n",
    "    (\"tqdm\", \"tqdm\"),\n",
    "    (\"numpy\", \"numpy\"),\n",
    "    (\"pydantic-settings\", \"pydantic_settings\"),\n",
    "    (\"pydantic\", \"pydantic\"),\n",
    "    (\"python-dotenv\", \"dotenv\"),\n",
    "    (\"tiktoken\", \"tiktoken\"),\n",
    "    (\"transformers\", \"transformers\"),\n",
    "    (\"torch\", \"torch\")\n",
    "]\n",
    "\n",
    "print(\"üîß Installing required packages...\")\n",
    "print(\"This may take several minutes for first-time installation.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "installed_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for package_name, import_name in required_packages:\n",
    "    if check_package(package_name, import_name):\n",
    "        print(f\"‚úÖ {package_name} already installed\")\n",
    "        installed_count += 1\n",
    "    else:\n",
    "        print(f\"üì¶ Installing {package_name}...\")\n",
    "        if install_package(package_name):\n",
    "            installed_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"üìä Installation Summary:\")\n",
    "print(f\"   ‚úÖ Installed/Available: {installed_count}\")\n",
    "print(f\"   ‚ùå Failed: {failed_count}\")\n",
    "\n",
    "if failed_count == 0:\n",
    "    print(\"‚úÖ All dependencies are ready!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Some packages failed to install. You may encounter import errors.\")\n",
    "\n",
    "print(\"üí° You can now run the rest of the notebook cells.\")\n",
    "\n",
    "# Verify critical imports\n",
    "print(\"\\nüîç Verifying critical imports...\")\n",
    "critical_imports = [\n",
    "    (\"chromadb\", \"ChromaDB for vector storage\"),\n",
    "    (\"sentence_transformers\", \"Sentence transformers for embeddings\"),\n",
    "    (\"PyPDF2\", \"PDF processing\"),\n",
    "    (\"numpy\", \"Numerical operations\")\n",
    "]\n",
    "\n",
    "all_critical_ok = True\n",
    "for import_name, description in critical_imports:\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"‚úÖ {import_name}: OK\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {import_name}: MISSING - {description}\")\n",
    "        all_critical_ok = False\n",
    "\n",
    "if all_critical_ok:\n",
    "    print(\"üéâ All critical dependencies verified!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some critical dependencies are missing. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb673c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special handling for ChromaDB and related dependencies\n",
    "# Run this cell if you still get import errors after the main installation\n",
    "\n",
    "print(\"üîß Installing ChromaDB and related dependencies...\")\n",
    "\n",
    "# ChromaDB often needs specific versions and additional dependencies\n",
    "chromadb_packages = [\n",
    "    \"chromadb>=0.4.0\",\n",
    "    \"hnswlib\",\n",
    "    \"sentence-transformers>=2.2.0\",\n",
    "    \"transformers>=4.30.0\",\n",
    "    \"torch\",\n",
    "    \"numpy<2.0.0\",  # ChromaDB may have issues with numpy 2.0+\n",
    "    \"pydantic>=2.0.0,<3.0.0\",\n",
    "    \"tiktoken\",\n",
    "    \"openai>=1.0.0\"\n",
    "]\n",
    "\n",
    "print(\"Installing ChromaDB ecosystem packages...\")\n",
    "for package in chromadb_packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ö†Ô∏è  {package}: {e}\")\n",
    "\n",
    "# Test ChromaDB specifically\n",
    "print(\"\\nüß™ Testing ChromaDB installation...\")\n",
    "try:\n",
    "    import chromadb\n",
    "    client = chromadb.Client()\n",
    "    print(\"‚úÖ ChromaDB is working correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ChromaDB test failed: {e}\")\n",
    "    print(\"üí° You may need to restart your kernel and try again\")\n",
    "\n",
    "print(\"‚úÖ ChromaDB setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Current directory: /Users/zsolt/Downloads/edu_platform/rag-pipeline\n",
      "üìÅ Source path added: /Users/zsolt/Downloads/edu_platform/rag-pipeline/src\n",
      "üîÑ Importing RAG pipeline components...\n",
      "‚ùå ChromaDB import failed: No module named 'chromadb'\n",
      "üí° Please run the dependency installation cell above\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chromadb'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîÑ Importing RAG pipeline components...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Try ChromaDB first as it's most likely to fail\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ ChromaDB imported successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'chromadb'"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Try to import optional dependencies with fallbacks\n",
    "try:\n",
    "    import numpy as np\n",
    "    HAS_NUMPY = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è NumPy not available - some features may be limited\")\n",
    "    HAS_NUMPY = False\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è tqdm not available - progress bars will be disabled\")\n",
    "    # Fallback tqdm that does nothing\n",
    "    def tqdm(iterable, *args, **kwargs):\n",
    "        return iterable\n",
    "    HAS_TQDM = False\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Add src directory to path for imports\n",
    "current_dir = Path.cwd()\n",
    "src_path = current_dir / \"src\" \n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"üìÇ Current directory: {current_dir}\")\n",
    "print(f\"üìÅ Source path added: {src_path}\")\n",
    "\n",
    "# Import RAG pipeline components with comprehensive error handling\n",
    "print(\"üîÑ Importing RAG pipeline components...\")\n",
    "\n",
    "# Test critical dependencies first\n",
    "missing_deps = []\n",
    "\n",
    "try:\n",
    "    import chromadb\n",
    "    print(\"‚úÖ ChromaDB imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ChromaDB import failed: {e}\")\n",
    "    missing_deps.append(\"chromadb\")\n",
    "\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    print(\"‚úÖ Sentence Transformers imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Sentence Transformers import failed: {e}\")\n",
    "    missing_deps.append(\"sentence-transformers\")\n",
    "\n",
    "try:\n",
    "    import PyPDF2\n",
    "    print(\"‚úÖ PyPDF2 imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå PyPDF2 import failed: {e}\")\n",
    "    missing_deps.append(\"PyPDF2\")\n",
    "\n",
    "if missing_deps:\n",
    "    print(f\"\\n‚ùå Missing critical dependencies: {', '.join(missing_deps)}\")\n",
    "    print(\"üí° Please run the dependency installation cells above\")\n",
    "    print(\"üí° You may need to restart your kernel after installation\")\n",
    "    \n",
    "    # Don't completely fail - let user try to install dependencies\n",
    "    print(\"\\n‚ö†Ô∏è Attempting to continue with limited functionality...\")\n",
    "else:\n",
    "    print(\"‚úÖ All critical dependencies are available\")\n",
    "\n",
    "# Try to import our custom components\n",
    "try:\n",
    "    from src import DocumentProcessor, DocumentChunk, VectorStore, EmbeddingService, get_settings\n",
    "    print(\"‚úÖ All RAG components imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Bundle import error: {e}\")\n",
    "    print(\"üìù Trying individual imports...\")\n",
    "    \n",
    "    # Try individual imports as fallback\n",
    "    components_loaded = {}\n",
    "    \n",
    "    try:\n",
    "        from src.document_processor import DocumentProcessor, DocumentChunk\n",
    "        components_loaded['DocumentProcessor'] = True\n",
    "        print(\"‚úÖ DocumentProcessor imported\")\n",
    "    except ImportError as e2:\n",
    "        print(f\"‚ùå DocumentProcessor import failed: {e2}\")\n",
    "        components_loaded['DocumentProcessor'] = False\n",
    "    \n",
    "    try:\n",
    "        from src.vector_store import VectorStore\n",
    "        components_loaded['VectorStore'] = True\n",
    "        print(\"‚úÖ VectorStore imported\")\n",
    "    except ImportError as e3:\n",
    "        print(f\"‚ùå VectorStore import failed: {e3}\")\n",
    "        components_loaded['VectorStore'] = False\n",
    "    \n",
    "    try:\n",
    "        from src.embeddings import EmbeddingService\n",
    "        components_loaded['EmbeddingService'] = True\n",
    "        print(\"‚úÖ EmbeddingService imported\")\n",
    "    except ImportError as e4:\n",
    "        print(f\"‚ùå EmbeddingService import failed: {e4}\")\n",
    "        components_loaded['EmbeddingService'] = False\n",
    "    \n",
    "    try:\n",
    "        from src.config import get_settings\n",
    "        components_loaded['get_settings'] = True\n",
    "        print(\"‚úÖ get_settings imported\")\n",
    "    except ImportError as e5:\n",
    "        print(f\"‚ùå get_settings import failed: {e5}\")\n",
    "        components_loaded['get_settings'] = False\n",
    "    \n",
    "    # Check what we successfully loaded\n",
    "    loaded_count = sum(components_loaded.values())\n",
    "    if loaded_count == len(components_loaded):\n",
    "        print(\"‚úÖ All individual components loaded successfully!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Only {loaded_count}/{len(components_loaded)} components loaded\")\n",
    "        print(\"üí° Make sure you're running this notebook from the rag-pipeline directory\")\n",
    "        print(\"üí° Also ensure you have run the dependency installation cells above\")\n",
    "\n",
    "print(\"\\nüì¶ Available components (if loaded successfully):\")\n",
    "print(\"  - DocumentProcessor: Handles document chunking and metadata extraction\") \n",
    "print(\"  - DocumentChunk: Container for processed document segments\")\n",
    "print(\"  - VectorStore: Manages ChromaDB storage and similarity search\")\n",
    "print(\"  - EmbeddingService: Generates embeddings using sentence transformers\")\n",
    "print(\"  - get_settings: Loads configuration from .env file\")\n",
    "\n",
    "# Final dependency check with helpful messages\n",
    "print(\"\\nüîç Final dependency verification:\")\n",
    "final_check = {\n",
    "    \"chromadb\": \"Vector database storage\",\n",
    "    \"sentence_transformers\": \"Text embeddings\",\n",
    "    \"PyPDF2\": \"PDF file processing\",\n",
    "    \"openai\": \"OpenAI API (optional)\",\n",
    "    \"numpy\": \"Numerical operations\"\n",
    "}\n",
    "\n",
    "all_good = True\n",
    "for package, description in final_check.items():\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package}: Available\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package}: Missing - {description}\")\n",
    "        all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\nüöÄ All systems ready! You can proceed to the next cells.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some dependencies are missing.\")\n",
    "    print(\"üí° Try running cells 2-3 to install missing packages\")\n",
    "    print(\"üí° You may need to restart your kernel after installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcdf77",
   "metadata": {},
   "source": [
    "## 2. Configuration and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbed96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from .env file\n",
    "try:\n",
    "    settings = get_settings()\n",
    "    print(\"‚úÖ Configuration loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not load .env file. Using default settings. Error: {e}\")\n",
    "    \n",
    "    # Fallback configuration\n",
    "    class Settings:\n",
    "        def __init__(self):\n",
    "            self.openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "            self.vector_db_path = \"../backend/chroma_db\"\n",
    "            self.collection_name = \"school_knowledge\"\n",
    "            self.local_embedding_model = \"all-MiniLM-L6-v2\"\n",
    "            self.use_openai_embeddings = False\n",
    "            self.chunk_size = 1000\n",
    "            self.chunk_overlap = 200\n",
    "            self.min_chunk_size = 100\n",
    "            self.batch_size = 32\n",
    "            self.documents_dir = \"./documents\"\n",
    "    \n",
    "    settings = Settings()\n",
    "\n",
    "# Display configuration\n",
    "print(\"\\nüìã Current Configuration:\")\n",
    "print(f\"üóÇÔ∏è Documents directory: {settings.documents_dir}\")\n",
    "print(f\"üóÉÔ∏è Vector DB path: {settings.vector_db_path}\")\n",
    "print(f\"üìö Collection name: {settings.collection_name}\")\n",
    "print(f\"ü§ñ Embedding model: {settings.local_embedding_model}\")\n",
    "print(f\"üî¢ Chunk size: {settings.chunk_size}\")\n",
    "print(f\"üìä Batch size: {settings.batch_size}\")\n",
    "print(f\"üîë OpenAI API key configured: {'Yes' if settings.openai_api_key else 'No'}\")\n",
    "\n",
    "# Create documents directory if it doesn't exist\n",
    "documents_path = Path(settings.documents_dir)\n",
    "documents_path.mkdir(exist_ok=True)\n",
    "print(f\"üìÅ Documents directory ready: {documents_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815a3d4",
   "metadata": {},
   "source": [
    "## 3. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed982391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize document processor with enhanced chapter detection\n",
    "print(\"üîÑ Initializing document processor with chapter-based splitting...\")\n",
    "doc_processor = DocumentProcessor(\n",
    "    chunk_size=settings.chunk_size,\n",
    "    chunk_overlap=settings.chunk_overlap,\n",
    "    min_chunk_size=settings.min_chunk_size\n",
    ")\n",
    "print(\"‚úÖ Document processor initialized with chapter detection\")\n",
    "\n",
    "# Initialize embedding service\n",
    "print(\"üîÑ Initializing embedding service...\")\n",
    "try:\n",
    "    embedding_service = EmbeddingService(\n",
    "        openai_api_key=settings.openai_api_key if hasattr(settings, 'openai_api_key') else None,\n",
    "        model_name=settings.local_embedding_model\n",
    "    )\n",
    "    print(\"‚úÖ Embedding service initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Embedding service initialization warning: {e}\")\n",
    "    print(\"üí° Some embedding features may be limited\")\n",
    "\n",
    "# Create a ChromaDB-compatible embedding function\n",
    "print(\"üîÑ Creating embedding function for ChromaDB...\")\n",
    "try:\n",
    "    import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "    # Create a sentence transformers embedding function that ChromaDB can use\n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=settings.local_embedding_model\n",
    "    )\n",
    "    print(\"‚úÖ ChromaDB embedding function created\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è ChromaDB embedding function creation warning: {e}\")\n",
    "    sentence_transformer_ef = None\n",
    "\n",
    "# Initialize vector store (connects to backend ChromaDB) WITH embedding function\n",
    "print(\"üîÑ Connecting to ChromaDB...\")\n",
    "try:\n",
    "    vector_store = VectorStore(\n",
    "        collection_name=settings.collection_name,\n",
    "        persist_directory=settings.vector_db_path,\n",
    "        embedding_function=sentence_transformer_ef  # Add the embedding function if available\n",
    "    )\n",
    "    print(\"‚úÖ Connected to ChromaDB successfully\")\n",
    "    print(f\"üìä Current document count: {vector_store.count_documents()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to ChromaDB: {e}\")\n",
    "    print(\"üí° This may be normal if this is your first run\")\n",
    "    print(\"üí° The vector store will be created when you upload documents\")\n",
    "    # Don't raise here, let the process continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4c615",
   "metadata": {},
   "source": [
    "## 4. Document Discovery and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_documents(documents_dir: str) -> List[Path]:\n",
    "    \"\"\"Discover all supported document files in the directory.\"\"\"\n",
    "    supported_extensions = {'.pdf', '.docx', '.doc', '.txt', '.md', '.markdown'}\n",
    "    documents = []\n",
    "    \n",
    "    documents_path = Path(documents_dir)\n",
    "    \n",
    "    for ext in supported_extensions:\n",
    "        pattern = f\"*{ext}\"\n",
    "        files = list(documents_path.glob(pattern))\n",
    "        documents.extend(files)\n",
    "        if files:\n",
    "            print(f\"üìÑ Found {len(files)} {ext} files\")\n",
    "    \n",
    "    return sorted(documents)\n",
    "\n",
    "# Discover documents in the documents folder\n",
    "print(\"üîç Discovering documents...\")\n",
    "document_files = discover_documents(settings.documents_dir)\n",
    "\n",
    "if not document_files:\n",
    "    print(f\"‚ö†Ô∏è No documents found in {settings.documents_dir}\")\n",
    "    print(\"üìù Supported formats: PDF, DOCX, DOC, TXT, MD\")\n",
    "    print(\"üí° Please add some documents to the documents folder and re-run this cell\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(document_files)} documents to process:\")\n",
    "    for doc in document_files:\n",
    "        size_mb = doc.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  üìÑ {doc.name} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850999c8",
   "metadata": {},
   "source": [
    "## 5. Document Processing and Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d473f1e",
   "metadata": {},
   "source": [
    "### üìñ Enhanced Chapter-Based Processing\n",
    "\n",
    "This notebook now includes **chapter-based document splitting** that:\n",
    "- **Detects Hungarian textbook chapters** automatically\n",
    "- **Preserves educational coherence** by keeping complete concepts together\n",
    "- **Enhances metadata** with chapter titles, topics, and educational context\n",
    "- **Improves AI responses** in both ai-chat and ai-tutor\n",
    "\n",
    "The processing will automatically detect if your documents have chapter structure and use the enhanced splitting method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2167b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "import time\n",
    "\n",
    "def process_document_with_metadata(file_path: Path) -> List[DocumentChunk]:\n",
    "    \"\"\"Process a document file and extract metadata from filename.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nüîÑ Processing: {file_path.name}\")\n",
    "        print(f\"üì¶ File size: {file_path.stat().st_size / (1024 * 1024):.1f} MB\")\n",
    "        \n",
    "        # Extract metadata from filename\n",
    "        print(\"üìã Extracting metadata from filename...\")\n",
    "        filename_metadata = doc_processor.extract_metadata_from_filename(file_path.name)\n",
    "        print(f\" Filename metadata: {filename_metadata}\")\n",
    "\n",
    "        # Add source metadata\n",
    "        source_metadata = {\n",
    "            'processed_at': datetime.now().isoformat(),\n",
    "            'file_path': str(file_path),\n",
    "            **filename_metadata\n",
    "        }\n",
    "        \n",
    "        # OPTIMIZED: Extract text only once and reuse it\n",
    "        print(f\"üìñ Reading document: {file_path.name}...\")\n",
    "        extraction_start = time.time()\n",
    "        \n",
    "        # Add timeout protection for text extraction\n",
    "        try:\n",
    "            full_text = doc_processor._extract_text(file_path)\n",
    "            extraction_time = time.time() - extraction_start\n",
    "            print(f\"‚è±Ô∏è Text extraction took: {extraction_time:.1f} seconds\")\n",
    "        except Exception as extract_error:\n",
    "            print(f\"‚ùå Text extraction failed: {extract_error}\")\n",
    "            return []\n",
    "        \n",
    "        if not full_text or len(full_text.strip()) == 0:\n",
    "            print(f\"‚ö†Ô∏è WARNING: No text extracted from {file_path.name}\")\n",
    "            return []\n",
    "        \n",
    "        # Calculate comprehensive document statistics\n",
    "        total_chars = len(full_text)\n",
    "        total_words = len(full_text.split())\n",
    "        total_lines = full_text.count('\\n') + 1\n",
    "        estimated_pages = total_words / 250  # Standard: ~250 words per page\n",
    "\n",
    "        print(f\"üìä Document analysis: {len(full_text)/1024:.1f} KB of text, {total_words:,} words\")\n",
    "        \n",
    "        # Add comprehensive document statistics to metadata\n",
    "        source_metadata.update({\n",
    "            'original_char_count': total_chars,\n",
    "            'original_word_count': total_words,\n",
    "            'original_line_count': total_lines,\n",
    "            'estimated_pages': round(estimated_pages, 1),\n",
    "            'text_size_kb': round(len(full_text)/1024, 2)\n",
    "        })\n",
    "        \n",
    "        # OPTIMIZED: Process the text directly instead of re-reading the file\n",
    "        print(f\"‚úÇÔ∏è Chunking document...\")\n",
    "        chunking_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            chunks = doc_processor.process_text(full_text, source_metadata)\n",
    "            chunking_time = time.time() - chunking_start\n",
    "            print(f\"‚è±Ô∏è Chunking took: {chunking_time:.1f} seconds\")\n",
    "        except Exception as chunk_error:\n",
    "            print(f\"‚ùå Chunking failed: {chunk_error}\")\n",
    "            return []\n",
    "        \n",
    "        if chunks:\n",
    "            # Calculate detailed chunking statistics\n",
    "            chunk_chars = sum(len(chunk.content) for chunk in chunks)\n",
    "            chunk_words = sum(len(chunk.content.split()) for chunk in chunks)\n",
    "            avg_chunk_size = chunk_chars / len(chunks) if chunks else 0\n",
    "            min_chunk_size = min(len(chunk.content) for chunk in chunks)\n",
    "            max_chunk_size = max(len(chunk.content) for chunk in chunks)\n",
    "            \n",
    "            print(f\"‚úÖ CHUNKING RESULTS:\")\n",
    "            print(f\"   üî¢ Chunks created: {len(chunks)}\")\n",
    "            print(f\"   üìè Average chunk size: {avg_chunk_size:.0f} chars\")\n",
    "            print(f\"   üìè Chunk size range: {min_chunk_size} - {max_chunk_size} chars\")\n",
    "            \n",
    "            # VERIFY METADATA IS ATTACHED\n",
    "            print(f\"üîç Metadata verification:\")\n",
    "            sample_chunk = chunks[0] if chunks else None\n",
    "            if sample_chunk and sample_chunk.metadata:\n",
    "                print(f\"   üìö Subject: {sample_chunk.metadata.get('subject', 'MISSING')}\")\n",
    "                print(f\"   üéì Grade: {sample_chunk.metadata.get('grade', 'MISSING')}\")\n",
    "                print(f\"   üìÑ Filename: {sample_chunk.metadata.get('filename', 'MISSING')}\")\n",
    "                print(f\"   üóÇÔ∏è Total metadata fields: {len(sample_chunk.metadata)}\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è WARNING: No metadata found in chunks!\")\n",
    "            \n",
    "            # Calculate and show text retention percentage\n",
    "            if full_text and len(full_text) > 0:\n",
    "                char_retention = (chunk_chars / len(full_text)) * 100\n",
    "                word_retention = (chunk_words / total_words) * 100 if total_words > 0 else 0\n",
    "                print(f\"   üìà Text retention: {char_retention:.1f}% chars, {word_retention:.1f}% words\")\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"‚úÖ Successfully processed: {file_path.name} in {total_time:.1f} seconds\")\n",
    "        else:\n",
    "            print(f\"‚ùå No chunks created for: {file_path.name}\")\n",
    "            print(\"   Check if the document has sufficient readable text content\")\n",
    "            \n",
    "        # Clear the full_text from memory to help with large files\n",
    "        del full_text\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR processing {file_path.name}: {str(e)}\")\n",
    "        print(f\"   Error type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Check if we have documents to process\n",
    "print(\"üîç Checking available documents...\")\n",
    "if 'document_files' not in locals() or not document_files:\n",
    "    print(\"‚ùå No document_files variable found. Please run the document discovery cell first.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(document_files)} documents to process\")\n",
    "\n",
    "# Process documents with better error handling and progress tracking\n",
    "if 'document_files' in locals() and document_files:\n",
    "    print(\"‚öôÔ∏è Processing ALL documents with optimized performance...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_chunks = []\n",
    "    processing_summary = []\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    # Sort files by size - process smaller files first for quick feedback\n",
    "    sorted_files = sorted(document_files, key=lambda f: f.stat().st_size)\n",
    "    print(f\"üìã Processing order (by size):\")\n",
    "    for i, doc_file in enumerate(sorted_files):\n",
    "        file_size_mb = doc_file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {i+1}. {doc_file.name} ({file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    for i, doc_file in enumerate(sorted_files, 1):\n",
    "        file_size_mb = doc_file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\nüìÑ DOCUMENT {i}/{len(sorted_files)} - {doc_file.name} ({file_size_mb:.1f} MB)\")\n",
    "        print(f\"üïê Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        doc_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            chunks = process_document_with_metadata(doc_file)\n",
    "            doc_time = time.time() - doc_start\n",
    "            \n",
    "            if chunks:\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "                # VERIFY METADATA PROPAGATION\n",
    "                first_chunk = chunks[0]\n",
    "                print(f\"üìã Metadata check: Subject='{first_chunk.metadata.get('subject')}', Grade='{first_chunk.metadata.get('grade')}', File='{first_chunk.metadata.get('filename')}'\")\n",
    "                \n",
    "                # Store individual document stats for summary\n",
    "                doc_stats = {\n",
    "                    'filename': doc_file.name,\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'processing_time': doc_time,\n",
    "                    'original_words': chunks[0].metadata.get('original_word_count', 0),\n",
    "                    'original_chars': chunks[0].metadata.get('original_char_count', 0),\n",
    "                    'original_lines': chunks[0].metadata.get('original_line_count', 0),\n",
    "                    'estimated_pages': chunks[0].metadata.get('estimated_pages', 0),\n",
    "                    'chunks_created': len(chunks),\n",
    "                    'processed_words': sum(len(chunk.content.split()) for chunk in chunks),\n",
    "                    'subject': chunks[0].metadata.get('subject', 'Unknown'),\n",
    "                    'grade': chunks[0].metadata.get('grade', 'Unknown')\n",
    "                }\n",
    "                processing_summary.append(doc_stats)\n",
    "                \n",
    "                print(f\"‚è±Ô∏è Document completed in {doc_time:.1f}s - Running total: {len(all_chunks):,} chunks\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No chunks generated from {doc_file.name}\")\n",
    "                \n",
    "        except Exception as doc_error:\n",
    "            print(f\"‚ùå Failed to process {doc_file.name}: {doc_error}\")\n",
    "            \n",
    "        print(\"-\" * 60)  # Separator between documents\n",
    "        \n",
    "        # Safety break - if any single document takes more than 5 minutes, something is wrong\n",
    "        if doc_time > 300:  # 5 minutes\n",
    "            print(f\"‚ö†Ô∏è WARNING: Document processing took {doc_time:.1f}s (>5min). This may indicate an issue.\")\n",
    "    \n",
    "    total_time = time.time() - overall_start\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üìä DOCUMENT STATISTICS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Show detailed individual document statistics\n",
    "    if processing_summary:\n",
    "        print(f\"{'Document':<35} {'Size(MB)':<8} {'Time(s)':<7} {'Chunks':<7} {'Subject':<12}\")\n",
    "        print(\"-\" * 85)\n",
    "        \n",
    "        for stats in processing_summary:\n",
    "            filename_short = stats['filename'][:32] + \"...\" if len(stats['filename']) > 35 else stats['filename']\n",
    "            \n",
    "            print(f\"{filename_short:<35} \"\n",
    "                  f\"{stats['file_size_mb']:<8.1f} \"\n",
    "                  f\"{stats['processing_time']:<7.1f} \"\n",
    "                  f\"{stats['chunks_created']:<7} \"\n",
    "                  f\"{stats['subject']:<12}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä COMPLETE PROCESSING SUMMARY:\")\n",
    "    print(f\"   üìö Documents processed: {len(document_files)}\")\n",
    "    print(f\"   üî¢ Total chunks created: {len(all_chunks):,}\")\n",
    "    print(f\"   ‚è±Ô∏è Total processing time: {total_time:.1f} seconds\")\n",
    "    if len(all_chunks) > 0 and total_time > 0:\n",
    "        print(f\"   üìà Average speed: {len(all_chunks)/total_time:.1f} chunks/second\")\n",
    "    \n",
    "    if processing_summary:\n",
    "        total_words = sum(s['original_words'] for s in processing_summary)\n",
    "        total_pages = sum(s['estimated_pages'] for s in processing_summary)\n",
    "        unique_subjects = set(s['subject'] for s in processing_summary)\n",
    "        unique_grades = set(s['grade'] for s in processing_summary)\n",
    "        \n",
    "        print(f\"   üî§ Total words across all docs: {total_words:,}\")\n",
    "        print(f\"   üìñ Total estimated pages: {total_pages:.1f}\")\n",
    "        print(f\"   üìö Unique subjects: {len(unique_subjects)} - {sorted(unique_subjects)}\")\n",
    "        print(f\"   üéì Unique grades: {len(unique_grades)} - {sorted(unique_grades)}\")\n",
    "    \n",
    "    # FINAL METADATA VERIFICATION\n",
    "    if all_chunks:\n",
    "        print(f\"\\nüîç FINAL METADATA VERIFICATION:\")\n",
    "        subjects_found = set()\n",
    "        grades_found = set()\n",
    "        files_found = set()\n",
    "\n",
    "        for chunk in all_chunks:  # Check all chunks\n",
    "            if chunk.metadata:\n",
    "                if chunk.metadata.get('subject'):\n",
    "                    subjects_found.add(chunk.metadata.get('subject'))\n",
    "                if chunk.metadata.get('grade'):\n",
    "                    grades_found.add(chunk.metadata.get('grade'))\n",
    "                if chunk.metadata.get('filename'):\n",
    "                    files_found.add(chunk.metadata.get('filename'))\n",
    "        \n",
    "        print(f\"   üìö Subjects in chunks: {sorted(subjects_found)}\")\n",
    "        print(f\"   üéì Grades in chunks: {sorted(grades_found)}\")\n",
    "        print(f\"   üìÑ Files in chunks: {len(files_found)} files\")\n",
    "        \n",
    "        if not subjects_found:\n",
    "            print(f\"   ‚ö†Ô∏è WARNING: No subjects found in chunk metadata!\")\n",
    "        if not grades_found:\n",
    "            print(f\"   ‚ö†Ô∏è WARNING: No grades found in chunk metadata!\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüéØ Ready to generate embeddings for {len(all_chunks):,} chunks from ALL documents...\")\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è No documents found to process\")\n",
    "    print(\"üí° Place PDF, DOCX, TXT, or MD files in the './documents/' folder\")\n",
    "    \n",
    "    # Set empty variables\n",
    "    all_chunks = []\n",
    "    processing_summary = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebf10b",
   "metadata": {},
   "source": [
    "## 6. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_embeddings_for_chunks(chunks: List[DocumentChunk]) -> List[np.ndarray]:\n",
    "    \"\"\"Generate embeddings for all document chunks.\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"‚ö†Ô∏è No chunks provided for embedding generation\")\n",
    "        return []\n",
    "    \n",
    "    # Extract text content from chunks\n",
    "    texts = [chunk.content for chunk in chunks]\n",
    "    \n",
    "    print(f\"üß† Generating embeddings for {len(texts)} chunks...\")\n",
    "    print(f\"üìä Estimated processing time: ~{len(texts) * 0.05:.1f} seconds\")\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    use_openai = hasattr(settings, 'use_openai_embeddings') and settings.use_openai_embeddings\n",
    "    \n",
    "    try:\n",
    "        embeddings = await embedding_service.embed_documents(\n",
    "            texts, \n",
    "            use_openai=use_openai, \n",
    "            batch_size=settings.batch_size\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Successfully generated {len(embeddings)} embeddings\")\n",
    "        \n",
    "        if embeddings:\n",
    "            embedding_dim = len(embeddings[0]) if embeddings else 0\n",
    "            print(f\"üìè Embedding dimension: {embedding_dim}\")\n",
    "            print(f\"üíæ Memory usage: ~{len(embeddings) * embedding_dim * 4 / 1024 / 1024:.1f} MB\")\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating embeddings: {e}\")\n",
    "        return []\n",
    "\n",
    "# Generate embeddings for ALL processed chunks\n",
    "if 'all_chunks' in locals() and all_chunks:\n",
    "    print(\"üîÑ Starting embedding generation for ALL documents...\")\n",
    "    embeddings = await generate_embeddings_for_chunks(all_chunks)\n",
    "    \n",
    "    if embeddings and len(embeddings) == len(all_chunks):\n",
    "        print(f\"‚úÖ Embedding generation successful!\")\n",
    "        print(f\"üìä Ready to upload {len(embeddings)} embeddings to ChromaDB\")\n",
    "    elif embeddings:\n",
    "        print(f\"‚ö†Ô∏è Partial success: {len(embeddings)} embeddings for {len(all_chunks)} chunks\")\n",
    "    else:\n",
    "        print(\"‚ùå No embeddings generated\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping embedding generation - no chunks available\")\n",
    "    print(\"üí° Make sure to run the document processing cell first\")\n",
    "    embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f41f6",
   "metadata": {},
   "source": [
    "## 7. Batch Upload to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_chromadb(chunks: List[DocumentChunk], embeddings: List[np.ndarray]):\n",
    "    \"\"\"Upload chunks and embeddings to ChromaDB.\"\"\"\n",
    "    if not chunks or not embeddings:\n",
    "        print(\"‚ö†Ô∏è No data to upload\")\n",
    "        return False\n",
    "    \n",
    "    if len(chunks) != len(embeddings):\n",
    "        print(f\"‚ùå Mismatch: {len(chunks)} chunks vs {len(embeddings)} embeddings\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"üì§ Uploading {len(chunks)} chunks to ChromaDB...\")\n",
    "    print(f\"üéØ Target database: {settings.vector_db_path}\")\n",
    "    print(f\"üìö Collection: {settings.collection_name}\")\n",
    "    \n",
    "    # Show initial database state\n",
    "    initial_count = vector_store.count_documents()\n",
    "    print(f\"üìä Initial document count: {initial_count}\")\n",
    "    \n",
    "    # VERIFY METADATA BEFORE UPLOAD\n",
    "    print(f\"\\nüîç PRE-UPLOAD METADATA VERIFICATION:\")\n",
    "    subjects_to_upload = set()\n",
    "    grades_to_upload = set()\n",
    "    files_to_upload = set()\n",
    "    \n",
    "    for chunk in chunks:  # Check all chunks\n",
    "        if chunk.metadata:\n",
    "            if chunk.metadata.get('subject'):\n",
    "                subjects_to_upload.add(chunk.metadata.get('subject'))\n",
    "            if chunk.metadata.get('grade'):\n",
    "                grades_to_upload.add(chunk.metadata.get('grade'))\n",
    "            if chunk.metadata.get('filename'):\n",
    "                files_to_upload.add(chunk.metadata.get('filename'))\n",
    "    \n",
    "    print(f\"   üìö Subjects to upload: {sorted(subjects_to_upload)}\")\n",
    "    print(f\"   üéì Grades to upload: {sorted(grades_to_upload)}\")\n",
    "    print(f\"   üìÑ Files to upload: {len(files_to_upload)} files\")\n",
    "    \n",
    "    if not subjects_to_upload:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: No subjects found in chunk metadata before upload!\")\n",
    "        print(f\"   üîç Sample chunk metadata: {chunks[0].metadata if chunks[0].metadata else 'EMPTY'}\")\n",
    "    \n",
    "    # Prepare data for ChromaDB\n",
    "    documents = [chunk.content for chunk in chunks]\n",
    "    metadatas = [chunk.metadata for chunk in chunks]\n",
    "    ids = [chunk.chunk_id for chunk in chunks]\n",
    "    \n",
    "    # VERIFY METADATA STRUCTURE\n",
    "    print(f\"\\nüîç METADATA STRUCTURE VERIFICATION:\")\n",
    "    sample_metadata = metadatas[0] if metadatas else {}\n",
    "    print(f\"   üìã Sample metadata keys: {list(sample_metadata.keys()) if sample_metadata else 'NO KEYS'}\")\n",
    "    print(f\"   üìö Sample subject: {sample_metadata.get('subject', 'MISSING') if sample_metadata else 'NO METADATA'}\")\n",
    "    print(f\"   üéì Sample grade: {sample_metadata.get('grade', 'MISSING') if sample_metadata else 'NO METADATA'}\")\n",
    "    print(f\"   üìÑ Sample filename: {sample_metadata.get('filename', 'MISSING') if sample_metadata else 'NO METADATA'}\")\n",
    "    \n",
    "    # Upload in batches with progress tracking\n",
    "    batch_size = settings.batch_size\n",
    "    total_uploaded = 0\n",
    "    failed_batches = 0\n",
    "    \n",
    "    print(f\"\\nüîÑ Uploading in batches of {batch_size}...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Uploading batches\"):\n",
    "        end_idx = min(i + batch_size, len(chunks))\n",
    "        \n",
    "        batch_documents = documents[i:end_idx]\n",
    "        batch_metadatas = metadatas[i:end_idx]\n",
    "        batch_embeddings = embeddings[i:end_idx]\n",
    "        batch_ids = ids[i:end_idx]\n",
    "        \n",
    "        try:\n",
    "            vector_store.add_documents(\n",
    "                documents=batch_documents,\n",
    "                metadatas=batch_metadatas,\n",
    "                embeddings=batch_embeddings,\n",
    "                ids=batch_ids\n",
    "            )\n",
    "            total_uploaded += len(batch_documents)\n",
    "            \n",
    "            # Progress update every 10 batches\n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"  üìà Progress: {total_uploaded}/{len(chunks)} chunks uploaded\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "            failed_batches += 1\n",
    "            continue\n",
    "    \n",
    "    # Final results\n",
    "    final_count = vector_store.count_documents()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Upload complete!\")\n",
    "    print(f\"üìä Results:\")\n",
    "    print(f\"  ‚Ä¢ Successfully uploaded: {total_uploaded} chunks\")\n",
    "    print(f\"  ‚Ä¢ Failed batches: {failed_batches}\")\n",
    "    print(f\"  ‚Ä¢ Database before: {initial_count} documents\")\n",
    "    print(f\"  ‚Ä¢ Database after: {final_count} documents\")\n",
    "    print(f\"  ‚Ä¢ Net increase: {final_count - initial_count} documents\")\n",
    "    \n",
    "    # POST-UPLOAD VERIFICATION - Sample more strategically\n",
    "    print(f\"\\nüîç POST-UPLOAD METADATA VERIFICATION:\")\n",
    "    try:\n",
    "        # Get a larger sample to ensure we get documents from all subjects\n",
    "        sample_docs = vector_store.get_documents_by_metadata({}, limit=100)\n",
    "        \n",
    "        if sample_docs and sample_docs.get('metadatas'):\n",
    "            uploaded_subjects = set()\n",
    "            uploaded_grades = set()\n",
    "            uploaded_files = set()\n",
    "            \n",
    "            print(f\"   üîç Checking {len(sample_docs.get('metadatas', []))} documents for metadata diversity...\")\n",
    "            \n",
    "            for metadata in sample_docs['metadatas']:\n",
    "                if metadata:\n",
    "                    if metadata.get('subject'):\n",
    "                        uploaded_subjects.add(metadata.get('subject'))\n",
    "                    if metadata.get('grade'):\n",
    "                        uploaded_grades.add(metadata.get('grade'))\n",
    "                    if metadata.get('filename'):\n",
    "                        uploaded_files.add(metadata.get('filename'))\n",
    "            \n",
    "            print(f\"   üìö Subjects in database: {sorted(uploaded_subjects)}\")\n",
    "            print(f\"   üéì Grades in database: {sorted(uploaded_grades)}\")\n",
    "            print(f\"   üìÑ Files in database: {len(uploaded_files)} files\")\n",
    "            \n",
    "            # Also check with specific subject queries to be sure\n",
    "            print(f\"   üìã Verifying each expected subject exists:\")\n",
    "            for expected_subject in sorted(subjects_to_upload):\n",
    "                subject_docs = vector_store.get_documents_by_metadata({'subject': expected_subject}, limit=1)\n",
    "                if subject_docs and subject_docs.get('metadatas') and len(subject_docs['metadatas']) > 0:\n",
    "                    print(f\"      ‚úÖ {expected_subject}: Found\")\n",
    "                else:\n",
    "                    print(f\"      ‚ùå {expected_subject}: Missing\")\n",
    "            \n",
    "            if uploaded_subjects == subjects_to_upload:\n",
    "                print(f\"   ‚úÖ Subject metadata preserved correctly!\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Subject metadata sampling issue detected!\")\n",
    "                print(f\"      Expected: {sorted(subjects_to_upload)}\")\n",
    "                print(f\"      Found in sample: {sorted(uploaded_subjects)}\")\n",
    "                print(f\"   üí° This may be due to document clustering - checking individual subjects above\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Could not retrieve documents for verification\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error during post-upload verification: {e}\")\n",
    "    \n",
    "    return total_uploaded > 0\n",
    "\n",
    "# Upload ALL processed data to ChromaDB\n",
    "if 'all_chunks' in locals() and 'embeddings' in locals() and all_chunks and embeddings:\n",
    "    print(\"üöÄ Starting complete upload to ChromaDB...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    success = upload_to_chromadb(all_chunks, embeddings)\n",
    "    \n",
    "    if success:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"üéâ UPLOAD SUCCESSFUL!\")\n",
    "        print(\"‚úÖ All documents have been processed and uploaded to ChromaDB\")\n",
    "        print(\"üéØ Your knowledge base is now ready for use\")\n",
    "        print(\"üí° You can now test the search functionality or use the backend API\")\n",
    "    else:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"‚ùå Upload failed or incomplete\")\n",
    "        print(\"üí° Check the error messages above and try again\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping upload - missing chunks or embeddings\")\n",
    "    print(\"üí° Make sure to run both the processing and embedding generation cells first\")\n",
    "    \n",
    "    if 'all_chunks' not in locals() or not all_chunks:\n",
    "        print(\"  ‚ùå No chunks available (run document processing cell)\")\n",
    "    if 'embeddings' not in locals() or not embeddings:\n",
    "        print(\"  ‚ùå No embeddings available (run embedding generation cell)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b33800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification of all subjects in the database\n",
    "print(\"üîç MANUAL SUBJECT VERIFICATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "expected_subjects = ['foldrajz', 'irodalom', 'matematika', 'nyelvtan', 'tortenelem']\n",
    "\n",
    "for subject in expected_subjects:\n",
    "    try:\n",
    "        # Check if documents with this subject exist\n",
    "        subject_docs = vector_store.get_documents_by_metadata({'subject': subject}, limit=5)\n",
    "        if subject_docs and subject_docs.get('metadatas') and len(subject_docs['metadatas']) > 0:\n",
    "            count = len(subject_docs['metadatas'])\n",
    "            sample_filename = subject_docs['metadatas'][0].get('filename', 'Unknown')\n",
    "            print(f\"‚úÖ {subject}: Found {count} documents (sample file: {sample_filename})\")\n",
    "        else:\n",
    "            print(f\"‚ùå {subject}: No documents found\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {subject}: Error checking - {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Also check total counts by getting all documents and counting subjects\n",
    "print(\"üìä COMPREHENSIVE DATABASE ANALYSIS:\")\n",
    "try:\n",
    "    # Get more documents to analyze the full distribution\n",
    "    all_sample_docs = vector_store.collection.get(limit=2843, include=[\"metadatas\"])  # Get all docs\n",
    "    \n",
    "    if all_sample_docs and all_sample_docs.get('metadatas'):\n",
    "        all_subjects = set()\n",
    "        all_files = set()\n",
    "        subject_counts = {}\n",
    "        \n",
    "        for metadata in all_sample_docs['metadatas']:\n",
    "            if metadata and metadata.get('subject'):\n",
    "                subject = metadata.get('subject')\n",
    "                all_subjects.add(subject)\n",
    "                subject_counts[subject] = subject_counts.get(subject, 0) + 1\n",
    "                \n",
    "            if metadata and metadata.get('filename'):\n",
    "                all_files.add(metadata.get('filename'))\n",
    "        \n",
    "        print(f\"üìö All subjects in database: {sorted(all_subjects)}\")\n",
    "        print(f\"üìÑ All files in database: {len(all_files)} files\")\n",
    "        print(f\"üìä Subject distribution:\")\n",
    "        for subject, count in sorted(subject_counts.items()):\n",
    "            print(f\"   ‚Ä¢ {subject}: {count:,} chunks\")\n",
    "        \n",
    "        total_chunks_with_subjects = sum(subject_counts.values())\n",
    "        print(f\"üìà Total chunks with subject metadata: {total_chunks_with_subjects:,} / {len(all_sample_docs['metadatas']):,}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Could not retrieve documents for analysis\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during comprehensive analysis: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4835c",
   "metadata": {},
   "source": [
    "## 8. Utility Functions\n",
    "\n",
    "Additional utility functions for managing the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b00c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_knowledge_base():\n",
    "    \"\"\"Reset (clear) the entire knowledge base. Use with caution!\"\"\"\n",
    "    confirm = input(\"‚ö†Ô∏è This will delete ALL documents from ChromaDB. Type 'CONFIRM' to proceed: \")\n",
    "    \n",
    "    if confirm == \"CONFIRM\":\n",
    "        try:\n",
    "            vector_store.reset_collection()\n",
    "            print(\"‚úÖ Knowledge base reset successfully\")\n",
    "            print(f\"üìä New document count: {vector_store.count_documents()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error resetting knowledge base: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå Reset cancelled\")\n",
    "\n",
    "def export_processing_report():\n",
    "    \"\"\"Export a processing report with statistics.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"configuration\": {\n",
    "            \"documents_dir\": settings.documents_dir,\n",
    "            \"vector_db_path\": settings.vector_db_path,\n",
    "            \"collection_name\": settings.collection_name,\n",
    "            \"chunk_size\": settings.chunk_size,\n",
    "            \"batch_size\": settings.batch_size\n",
    "        },\n",
    "        \"processing_results\": {\n",
    "            \"documents_found\": len(document_files) if 'document_files' in locals() else 0,\n",
    "            \"chunks_created\": len(all_chunks) if 'all_chunks' in locals() else 0,\n",
    "            \"embeddings_generated\": len(embeddings) if 'embeddings' in locals() else 0,\n",
    "            \"final_db_count\": vector_store.count_documents()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    report_file = f\"processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"üìä Processing report exported to: {report_file}\")\n",
    "    return report\n",
    "\n",
    "# Uncomment the following lines to use these utilities:\n",
    "\n",
    "# Reset knowledge base (CAUTION!)\n",
    "#reset_knowledge_base()\n",
    "\n",
    "# Export processing report\n",
    "report = export_processing_report()\n",
    "print(f\"\\nüìã Processing Summary:\")\n",
    "print(f\"  Documents processed: {report['processing_results']['documents_found']}\")\n",
    "print(f\"  Chunks created: {report['processing_results']['chunks_created']}\")\n",
    "print(f\"  Total documents in DB: {report['processing_results']['final_db_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick diagnostic - let's check what's actually in the database by subject\n",
    "print(\"üîç Checking database contents by subject...\")\n",
    "\n",
    "# Get a larger sample to see all subjects\n",
    "collection = vector_store.collection\n",
    "sample = collection.get(limit=1000)  # Get more documents\n",
    "\n",
    "if sample and 'metadatas' in sample:\n",
    "    subject_counts = {}\n",
    "    file_counts = {}\n",
    "    \n",
    "    for metadata in sample['metadatas']:\n",
    "        subject = metadata.get('subject', 'unknown')\n",
    "        filename = metadata.get('filename', 'unknown')\n",
    "        \n",
    "        subject_counts[subject] = subject_counts.get(subject, 0) + 1\n",
    "        file_counts[filename] = file_counts.get(filename, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä Subject distribution in database:\")\n",
    "    for subject, count in sorted(subject_counts.items()):\n",
    "        print(f\"  üìö {subject}: {count} chunks\")\n",
    "    \n",
    "    print(f\"\\nüìÑ File distribution in database:\")\n",
    "    for filename, count in sorted(file_counts.items()):\n",
    "        print(f\"  üìÑ {filename}: {count} chunks\")\n",
    "        \n",
    "    print(f\"\\nüìà Total chunks in database: {sum(subject_counts.values())}\")\n",
    "    \n",
    "    # Test retrieval with OpenAI embeddings\n",
    "    print(f\"\\nüîç Testing Hungarian query with OpenAI embeddings...\")\n",
    "    try:\n",
    "        # Test with the embedding service using OpenAI\n",
    "        embedding_service = EmbeddingService(\n",
    "            openai_api_key=settings.openai_api_key,\n",
    "            model_name=settings.local_embedding_model\n",
    "        )\n",
    "        \n",
    "        # Test query embedding using OpenAI\n",
    "        test_query = \"mi a matematika?\"\n",
    "        print(f\"üîÑ Generating OpenAI embedding for query: '{test_query}'\")\n",
    "        \n",
    "        # Use async embedding with OpenAI\n",
    "        import asyncio\n",
    "        \n",
    "        async def test_openai_embedding():\n",
    "            query_embedding = await embedding_service.embed_text(test_query, use_openai=True)\n",
    "            return query_embedding\n",
    "        \n",
    "        # Run the async function\n",
    "        query_embedding = await test_openai_embedding()\n",
    "        print(f\"‚úÖ OpenAI query embedding generated: shape {query_embedding.shape}\")\n",
    "        \n",
    "        # Search directly in vector store\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=3,\n",
    "            include=['metadatas', 'documents', 'distances']\n",
    "        )\n",
    "        \n",
    "        if results and results['documents']:\n",
    "            print(f\"üîç Found {len(results['documents'][0])} results for 'mi a matematika?' using OpenAI embeddings:\")\n",
    "            for i, (doc, metadata, distance) in enumerate(zip(\n",
    "                results['documents'][0], \n",
    "                results['metadatas'][0], \n",
    "                results['distances'][0]\n",
    "            )):\n",
    "                subject = metadata.get('subject', 'unknown')\n",
    "                filename = metadata.get('filename', 'unknown')\n",
    "                print(f\"  {i+1}. {subject} ({filename}) - distance: {distance:.3f}\")\n",
    "                print(f\"     üìù {doc[:80]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during OpenAI embedding retrieval test: {e}\")\n",
    "        print(f\"üí° Make sure your OpenAI API key is configured correctly\")\n",
    "        \n",
    "        # Fallback to local embeddings if OpenAI fails\n",
    "        print(f\"\\nüîÑ Falling back to local embeddings...\")\n",
    "        try:\n",
    "            query_embedding = embedding_service._embed_with_local_model(test_query)\n",
    "            print(f\"‚úÖ Local query embedding generated: shape {query_embedding.shape}\")\n",
    "            \n",
    "            # Search with local embedding\n",
    "            results = collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=3,\n",
    "                include=['metadatas', 'documents', 'distances']\n",
    "            )\n",
    "            \n",
    "            if results and results['documents']:\n",
    "                print(f\"üîç Found {len(results['documents'][0])} results using local embeddings:\")\n",
    "                for i, (doc, metadata, distance) in enumerate(zip(\n",
    "                    results['documents'][0], \n",
    "                    results['metadatas'][0], \n",
    "                    results['distances'][0]\n",
    "                )):\n",
    "                    subject = metadata.get('subject', 'unknown')\n",
    "                    filename = metadata.get('filename', 'unknown')\n",
    "                    print(f\"  {i+1}. {subject} ({filename}) - distance: {distance:.3f}\")\n",
    "                    print(f\"     üìù {doc[:80]}...\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Local embedding fallback also failed: {e2}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No metadata found in database\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
