{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e54dcb",
   "metadata": {},
   "source": [
    "# RAG Document Processing Pipeline\n",
    "\n",
    "This notebook processes educational documents in batch from a local folder and uploads them to ChromaDB for use by the backend system.\n",
    "\n",
    "## Features\n",
    "- Processes PDF, DOCX, TXT, and Markdown files\n",
    "- Automatic metadata extraction from filenames\n",
    "- Document chunking with overlap\n",
    "- Embedding generation using sentence transformers\n",
    "- Batch upload to ChromaDB with progress tracking\n",
    "- Error handling and logging\n",
    "\n",
    "## Usage\n",
    "1. Place your documents in the `./documents/` folder\n",
    "2. Configure the settings below\n",
    "3. Run all cells to process and upload documents\n",
    "4. Verify results using the testing section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21664ccd",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Add src directory to path for imports\n",
    "current_dir = Path.cwd()\n",
    "src_path = current_dir / \"src\" \n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"📂 Current directory: {current_dir}\")\n",
    "print(f\"📁 Source path added: {src_path}\")\n",
    "\n",
    "# Import RAG pipeline components\n",
    "try:\n",
    "    from src import DocumentProcessor, DocumentChunk, VectorStore, EmbeddingService, get_settings\n",
    "    print(\"✅ All RAG components imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Import error: {e}\")\n",
    "    print(\"📝 Trying individual imports...\")\n",
    "    \n",
    "    # Try individual imports as fallback\n",
    "    try:\n",
    "        from src.document_processor import DocumentProcessor, DocumentChunk\n",
    "        from src.vector_store import VectorStore  \n",
    "        from src.embeddings import EmbeddingService\n",
    "        from src.config import get_settings\n",
    "        print(\"✅ Individual imports successful!\")\n",
    "    except ImportError as e2:\n",
    "        print(f\"❌ Import failed: {e2}\")\n",
    "        print(\"💡 Make sure you're running this notebook from the rag-pipeline directory\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n📦 Available components:\")\n",
    "print(\"  - DocumentProcessor: Handles document chunking and metadata extraction\") \n",
    "print(\"  - DocumentChunk: Container for processed document segments\")\n",
    "print(\"  - VectorStore: Manages ChromaDB storage and similarity search\")\n",
    "print(\"  - EmbeddingService: Generates embeddings using sentence transformers\")\n",
    "print(\"  - get_settings: Loads configuration from .env file\")\n",
    "print(\"\\n🚀 Ready to process documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcdf77",
   "metadata": {},
   "source": [
    "## 2. Configuration and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbed96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from .env file\n",
    "try:\n",
    "    settings = get_settings()\n",
    "    print(\"✅ Configuration loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Warning: Could not load .env file. Using default settings. Error: {e}\")\n",
    "    \n",
    "    # Fallback configuration\n",
    "    class Settings:\n",
    "        def __init__(self):\n",
    "            self.openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "            self.vector_db_path = \"../backend/chroma_db\"\n",
    "            self.collection_name = \"school_knowledge\"\n",
    "            self.local_embedding_model = \"all-MiniLM-L6-v2\"\n",
    "            self.use_openai_embeddings = False\n",
    "            self.chunk_size = 1000\n",
    "            self.chunk_overlap = 200\n",
    "            self.min_chunk_size = 100\n",
    "            self.batch_size = 32\n",
    "            self.documents_dir = \"./documents\"\n",
    "    \n",
    "    settings = Settings()\n",
    "\n",
    "# Display configuration\n",
    "print(\"\\n📋 Current Configuration:\")\n",
    "print(f\"🗂️ Documents directory: {settings.documents_dir}\")\n",
    "print(f\"🗃️ Vector DB path: {settings.vector_db_path}\")\n",
    "print(f\"📚 Collection name: {settings.collection_name}\")\n",
    "print(f\"🤖 Embedding model: {settings.local_embedding_model}\")\n",
    "print(f\"🔢 Chunk size: {settings.chunk_size}\")\n",
    "print(f\"📊 Batch size: {settings.batch_size}\")\n",
    "print(f\"🔑 OpenAI API key configured: {'Yes' if settings.openai_api_key else 'No'}\")\n",
    "\n",
    "# Create documents directory if it doesn't exist\n",
    "documents_path = Path(settings.documents_dir)\n",
    "documents_path.mkdir(exist_ok=True)\n",
    "print(f\"📁 Documents directory ready: {documents_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815a3d4",
   "metadata": {},
   "source": [
    "## 3. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed982391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize document processor\n",
    "print(\"🔄 Initializing document processor...\")\n",
    "doc_processor = DocumentProcessor(\n",
    "    chunk_size=settings.chunk_size,\n",
    "    chunk_overlap=settings.chunk_overlap,\n",
    "    min_chunk_size=settings.min_chunk_size\n",
    ")\n",
    "print(\"✅ Document processor initialized\")\n",
    "\n",
    "# Initialize embedding service\n",
    "print(\"🔄 Initializing embedding service...\")\n",
    "embedding_service = EmbeddingService(\n",
    "    openai_api_key=settings.openai_api_key if hasattr(settings, 'openai_api_key') else None,\n",
    "    model_name=settings.local_embedding_model\n",
    ")\n",
    "print(\"✅ Embedding service initialized\")\n",
    "\n",
    "# Initialize vector store (connects to backend ChromaDB)\n",
    "print(\"🔄 Connecting to ChromaDB...\")\n",
    "try:\n",
    "    vector_store = VectorStore(\n",
    "        collection_name=settings.collection_name,\n",
    "        persist_directory=settings.vector_db_path\n",
    "    )\n",
    "    print(\"✅ Connected to ChromaDB successfully\")\n",
    "    print(f\"📊 Current document count: {vector_store.count_documents()}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to ChromaDB: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4c615",
   "metadata": {},
   "source": [
    "## 4. Document Discovery and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_documents(documents_dir: str) -> List[Path]:\n",
    "    \"\"\"Discover all supported document files in the directory.\"\"\"\n",
    "    supported_extensions = {'.pdf', '.docx', '.doc', '.txt', '.md', '.markdown'}\n",
    "    documents = []\n",
    "    \n",
    "    documents_path = Path(documents_dir)\n",
    "    \n",
    "    for ext in supported_extensions:\n",
    "        pattern = f\"*{ext}\"\n",
    "        files = list(documents_path.glob(pattern))\n",
    "        documents.extend(files)\n",
    "        if files:\n",
    "            print(f\"📄 Found {len(files)} {ext} files\")\n",
    "    \n",
    "    return sorted(documents)\n",
    "\n",
    "# Discover documents in the documents folder\n",
    "print(\"🔍 Discovering documents...\")\n",
    "document_files = discover_documents(settings.documents_dir)\n",
    "\n",
    "if not document_files:\n",
    "    print(f\"⚠️ No documents found in {settings.documents_dir}\")\n",
    "    print(\"📝 Supported formats: PDF, DOCX, DOC, TXT, MD\")\n",
    "    print(\"💡 Please add some documents to the documents folder and re-run this cell\")\n",
    "else:\n",
    "    print(f\"✅ Found {len(document_files)} documents to process:\")\n",
    "    for doc in document_files:\n",
    "        size_mb = doc.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  📄 {doc.name} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850999c8",
   "metadata": {},
   "source": [
    "## 5. Document Processing and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2167b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_with_metadata(file_path: Path) -> List[DocumentChunk]:\n",
    "    \"\"\"Process a document file and extract metadata from filename.\"\"\"\n",
    "    \n",
    "    # Extract metadata from filename\n",
    "    filename_metadata = doc_processor.extract_metadata_from_filename(file_path.name)\n",
    "    \n",
    "    # Add source metadata\n",
    "    source_metadata = {\n",
    "        'processed_at': datetime.now().isoformat(),\n",
    "        'file_path': str(file_path),\n",
    "        **filename_metadata\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Process the document\n",
    "        chunks = doc_processor.process_file(file_path, source_metadata)\n",
    "        print(f\"  ✅ {file_path.name}: {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ {file_path.name}: Error - {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Process all documents\n",
    "if document_files:\n",
    "    print(\"⚙️ Processing documents...\")\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc_file in tqdm(document_files, desc=\"Processing documents\"):\n",
    "        chunks = process_document_with_metadata(doc_file)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"\\n✅ Processing complete!\")\n",
    "    print(f\"📊 Total documents processed: {len(document_files)}\")\n",
    "    print(f\"📊 Total chunks created: {len(all_chunks)}\")\n",
    "    \n",
    "    if all_chunks:\n",
    "        avg_chunk_size = sum(len(chunk.content) for chunk in all_chunks) / len(all_chunks)\n",
    "        print(f\"📊 Average chunk size: {avg_chunk_size:.0f} characters\")\n",
    "        \n",
    "        # Show sample metadata\n",
    "        print(f\"\\n📋 Sample metadata from first chunk:\")\n",
    "        sample_metadata = all_chunks[0].metadata\n",
    "        for key, value in sample_metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"⏭️ Skipping processing - no documents found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebf10b",
   "metadata": {},
   "source": [
    "## 6. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_embeddings_for_chunks(chunks: List[DocumentChunk]) -> List[np.ndarray]:\n",
    "    \"\"\"Generate embeddings for all document chunks.\"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    # Extract text content from chunks\n",
    "    texts = [chunk.content for chunk in chunks]\n",
    "    \n",
    "    print(f\"🧠 Generating embeddings for {len(texts)} chunks...\")\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    use_openai = hasattr(settings, 'use_openai_embeddings') and settings.use_openai_embeddings\n",
    "    \n",
    "    embeddings = await embedding_service.embed_documents(\n",
    "        texts, \n",
    "        use_openai=use_openai, \n",
    "        batch_size=settings.batch_size\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Generated {len(embeddings)} embeddings\")\n",
    "    return embeddings\n",
    "\n",
    "# Generate embeddings if we have chunks\n",
    "if 'all_chunks' in locals() and all_chunks:\n",
    "    print(\"🔄 Starting embedding generation...\")\n",
    "    embeddings = await generate_embeddings_for_chunks(all_chunks)\n",
    "    \n",
    "    if embeddings:\n",
    "        embedding_dim = len(embeddings[0]) if embeddings else 0\n",
    "        print(f\"📊 Embedding dimension: {embedding_dim}\")\n",
    "        print(f\"📊 Total embeddings: {len(embeddings)}\")\n",
    "    else:\n",
    "        print(\"❌ No embeddings generated\")\n",
    "else:\n",
    "    print(\"⏭️ Skipping embedding generation - no chunks available\")\n",
    "    embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f41f6",
   "metadata": {},
   "source": [
    "## 7. Batch Upload to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_chromadb(chunks: List[DocumentChunk], embeddings: List[np.ndarray]):\n",
    "    \"\"\"Upload chunks and embeddings to ChromaDB.\"\"\"\n",
    "    if not chunks or not embeddings:\n",
    "        print(\"⚠️ No data to upload\")\n",
    "        return\n",
    "    \n",
    "    if len(chunks) != len(embeddings):\n",
    "        print(f\"❌ Mismatch: {len(chunks)} chunks vs {len(embeddings)} embeddings\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📤 Uploading {len(chunks)} chunks to ChromaDB...\")\n",
    "    \n",
    "    # Prepare data for ChromaDB\n",
    "    documents = [chunk.content for chunk in chunks]\n",
    "    metadatas = [chunk.metadata for chunk in chunks]\n",
    "    ids = [chunk.chunk_id for chunk in chunks]\n",
    "    \n",
    "    # Upload in batches\n",
    "    batch_size = settings.batch_size\n",
    "    total_uploaded = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Uploading batches\"):\n",
    "        end_idx = min(i + batch_size, len(chunks))\n",
    "        \n",
    "        batch_documents = documents[i:end_idx]\n",
    "        batch_metadatas = metadatas[i:end_idx]\n",
    "        batch_embeddings = embeddings[i:end_idx]\n",
    "        batch_ids = ids[i:end_idx]\n",
    "        \n",
    "        try:\n",
    "            vector_store.add_documents(\n",
    "                documents=batch_documents,\n",
    "                metadatas=batch_metadatas,\n",
    "                embeddings=batch_embeddings,\n",
    "                ids=batch_ids\n",
    "            )\n",
    "            total_uploaded += len(batch_documents)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"✅ Upload complete! {total_uploaded} chunks uploaded\")\n",
    "    print(f\"📊 New total document count: {vector_store.count_documents()}\")\n",
    "\n",
    "# Upload data if available\n",
    "if 'all_chunks' in locals() and 'embeddings' in locals() and all_chunks and embeddings:\n",
    "    print(\"🚀 Starting upload to ChromaDB...\")\n",
    "    upload_to_chromadb(all_chunks, embeddings)\n",
    "else:\n",
    "    print(\"⏭️ Skipping upload - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243aadb3",
   "metadata": {},
   "source": [
    "## 8. Verification and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b883b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_search_functionality():\n",
    "    \"\"\"Test the search functionality of the uploaded documents.\"\"\"\n",
    "    \n",
    "    print(\"🔍 Testing search functionality...\")\n",
    "    \n",
    "    # Get some statistics\n",
    "    total_docs = vector_store.count_documents()\n",
    "    print(f\"📊 Total documents in ChromaDB: {total_docs}\")\n",
    "    \n",
    "    if total_docs == 0:\n",
    "        print(\"⚠️ No documents to search\")\n",
    "        return\n",
    "    \n",
    "    # Test basic search\n",
    "    test_queries = [\n",
    "        \"mathematics\",\n",
    "        \"science\",\n",
    "        \"history\", \n",
    "        \"algebra\",\n",
    "        \"biology\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n🔍 Testing query: '{query}'\")\n",
    "        try:\n",
    "            results = vector_store.similarity_search(query, n_results=3)\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                print(f\"  ✅ Found {len(results['documents'][0])} results\")\n",
    "                \n",
    "                # Show first result\n",
    "                first_doc = results['documents'][0][0]\n",
    "                first_metadata = results['metadatas'][0][0] if results['metadatas'][0] else {}\n",
    "                first_distance = results['distances'][0][0] if results['distances'][0] else 0\n",
    "                \n",
    "                print(f\"  📄 Top result (distance: {first_distance:.3f}):\")\n",
    "                print(f\"    Subject: {first_metadata.get('subject', 'Unknown')}\")\n",
    "                print(f\"    Grade: {first_metadata.get('grade', 'Unknown')}\")\n",
    "                print(f\"    Filename: {first_metadata.get('filename', 'Unknown')}\")\n",
    "                print(f\"    Preview: {first_doc[:100]}...\")\n",
    "            else:\n",
    "                print(\"  ❌ No results found\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Search error: {e}\")\n",
    "\n",
    "def get_knowledge_base_stats():\n",
    "    \"\"\"Get statistics about the knowledge base.\"\"\"\n",
    "    print(\"📊 Knowledge Base Statistics:\")\n",
    "    \n",
    "    try:\n",
    "        # Get all documents\n",
    "        all_docs = vector_store.get_documents_by_metadata({})\n",
    "        \n",
    "        if not all_docs['metadatas']:\n",
    "            print(\"  No metadata available\")\n",
    "            return\n",
    "        \n",
    "        metadatas = all_docs['metadatas']\n",
    "        \n",
    "        # Count subjects\n",
    "        subjects = set()\n",
    "        grades = set()\n",
    "        files = set()\n",
    "        \n",
    "        for metadata in metadatas:\n",
    "            if 'subject' in metadata:\n",
    "                subjects.add(metadata['subject'])\n",
    "            if 'grade' in metadata:\n",
    "                grades.add(metadata['grade'])\n",
    "            if 'filename' in metadata:\n",
    "                files.add(metadata['filename'])\n",
    "        \n",
    "        print(f\"  📚 Unique subjects: {len(subjects)} - {sorted(subjects)}\")\n",
    "        print(f\"  🎓 Unique grades: {len(grades)} - {sorted(grades)}\")\n",
    "        print(f\"  📄 Unique files: {len(files)}\")\n",
    "        print(f\"  📊 Total chunks: {len(metadatas)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error getting stats: {e}\")\n",
    "\n",
    "# Run verification tests\n",
    "print(\"🧪 Running verification tests...\")\n",
    "get_knowledge_base_stats()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "test_search_functionality()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4835c",
   "metadata": {},
   "source": [
    "## 9. Utility Functions\n",
    "\n",
    "Additional utility functions for managing the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b00c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_knowledge_base():\n",
    "    \"\"\"Reset (clear) the entire knowledge base. Use with caution!\"\"\"\n",
    "    confirm = input(\"⚠️ This will delete ALL documents from ChromaDB. Type 'CONFIRM' to proceed: \")\n",
    "    \n",
    "    if confirm == \"CONFIRM\":\n",
    "        try:\n",
    "            vector_store.reset_collection()\n",
    "            print(\"✅ Knowledge base reset successfully\")\n",
    "            print(f\"📊 New document count: {vector_store.count_documents()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error resetting knowledge base: {e}\")\n",
    "    else:\n",
    "        print(\"❌ Reset cancelled\")\n",
    "\n",
    "def export_processing_report():\n",
    "    \"\"\"Export a processing report with statistics.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"configuration\": {\n",
    "            \"documents_dir\": settings.documents_dir,\n",
    "            \"vector_db_path\": settings.vector_db_path,\n",
    "            \"collection_name\": settings.collection_name,\n",
    "            \"chunk_size\": settings.chunk_size,\n",
    "            \"batch_size\": settings.batch_size\n",
    "        },\n",
    "        \"processing_results\": {\n",
    "            \"documents_found\": len(document_files) if 'document_files' in locals() else 0,\n",
    "            \"chunks_created\": len(all_chunks) if 'all_chunks' in locals() else 0,\n",
    "            \"embeddings_generated\": len(embeddings) if 'embeddings' in locals() else 0,\n",
    "            \"final_db_count\": vector_store.count_documents()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    report_file = f\"processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"📊 Processing report exported to: {report_file}\")\n",
    "    return report\n",
    "\n",
    "# Uncomment the following lines to use these utilities:\n",
    "\n",
    "# Reset knowledge base (CAUTION!)\n",
    "# reset_knowledge_base()\n",
    "\n",
    "# Export processing report\n",
    "report = export_processing_report()\n",
    "print(f\"\\n📋 Processing Summary:\")\n",
    "print(f\"  Documents processed: {report['processing_results']['documents_found']}\")\n",
    "print(f\"  Chunks created: {report['processing_results']['chunks_created']}\")\n",
    "print(f\"  Total documents in DB: {report['processing_results']['final_db_count']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
