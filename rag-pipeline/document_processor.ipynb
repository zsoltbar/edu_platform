{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e54dcb",
   "metadata": {},
   "source": [
    "# RAG Document Processing Pipeline\n",
    "\n",
    "This notebook processes educational documents in batch from a local folder and uploads them to ChromaDB for use by the backend system.\n",
    "\n",
    "## Features\n",
    "- Processes PDF, DOCX, TXT, and Markdown files\n",
    "- Automatic metadata extraction from filenames\n",
    "- Document chunking with overlap\n",
    "- Embedding generation using sentence transformers\n",
    "- Batch upload to ChromaDB with progress tracking\n",
    "- Error handling and logging\n",
    "\n",
    "## Usage\n",
    "1. Place your documents in the `./documents/` folder\n",
    "2. Configure the settings below\n",
    "3. Run all cells to process and upload documents\n",
    "4. Verify results using the testing section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21664ccd",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Add src directory to path for imports\n",
    "current_dir = Path.cwd()\n",
    "src_path = current_dir / \"src\" \n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"📂 Current directory: {current_dir}\")\n",
    "print(f\"📁 Source path added: {src_path}\")\n",
    "\n",
    "# Import RAG pipeline components\n",
    "try:\n",
    "    from src import DocumentProcessor, DocumentChunk, VectorStore, EmbeddingService, get_settings\n",
    "    print(\"✅ All RAG components imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Import error: {e}\")\n",
    "    print(\"📝 Trying individual imports...\")\n",
    "    \n",
    "    # Try individual imports as fallback\n",
    "    try:\n",
    "        from src.document_processor import DocumentProcessor, DocumentChunk\n",
    "        from src.vector_store import VectorStore  \n",
    "        from src.embeddings import EmbeddingService\n",
    "        from src.config import get_settings\n",
    "        print(\"✅ Individual imports successful!\")\n",
    "    except ImportError as e2:\n",
    "        print(f\"❌ Import failed: {e2}\")\n",
    "        print(\"💡 Make sure you're running this notebook from the rag-pipeline directory\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n📦 Available components:\")\n",
    "print(\"  - DocumentProcessor: Handles document chunking and metadata extraction\") \n",
    "print(\"  - DocumentChunk: Container for processed document segments\")\n",
    "print(\"  - VectorStore: Manages ChromaDB storage and similarity search\")\n",
    "print(\"  - EmbeddingService: Generates embeddings using sentence transformers\")\n",
    "print(\"  - get_settings: Loads configuration from .env file\")\n",
    "print(\"\\n🚀 Ready to process documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcdf77",
   "metadata": {},
   "source": [
    "## 2. Configuration and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbed96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from .env file\n",
    "try:\n",
    "    settings = get_settings()\n",
    "    print(\"✅ Configuration loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Warning: Could not load .env file. Using default settings. Error: {e}\")\n",
    "    \n",
    "    # Fallback configuration\n",
    "    class Settings:\n",
    "        def __init__(self):\n",
    "            self.openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "            self.vector_db_path = \"../backend/chroma_db\"\n",
    "            self.collection_name = \"school_knowledge\"\n",
    "            self.local_embedding_model = \"all-MiniLM-L6-v2\"\n",
    "            self.use_openai_embeddings = False\n",
    "            self.chunk_size = 1000\n",
    "            self.chunk_overlap = 200\n",
    "            self.min_chunk_size = 100\n",
    "            self.batch_size = 32\n",
    "            self.documents_dir = \"./documents\"\n",
    "    \n",
    "    settings = Settings()\n",
    "\n",
    "# Display configuration\n",
    "print(\"\\n📋 Current Configuration:\")\n",
    "print(f\"🗂️ Documents directory: {settings.documents_dir}\")\n",
    "print(f\"🗃️ Vector DB path: {settings.vector_db_path}\")\n",
    "print(f\"📚 Collection name: {settings.collection_name}\")\n",
    "print(f\"🤖 Embedding model: {settings.local_embedding_model}\")\n",
    "print(f\"🔢 Chunk size: {settings.chunk_size}\")\n",
    "print(f\"📊 Batch size: {settings.batch_size}\")\n",
    "print(f\"🔑 OpenAI API key configured: {'Yes' if settings.openai_api_key else 'No'}\")\n",
    "\n",
    "# Create documents directory if it doesn't exist\n",
    "documents_path = Path(settings.documents_dir)\n",
    "documents_path.mkdir(exist_ok=True)\n",
    "print(f\"📁 Documents directory ready: {documents_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815a3d4",
   "metadata": {},
   "source": [
    "## 3. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed982391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize document processor\n",
    "print(\"🔄 Initializing document processor...\")\n",
    "doc_processor = DocumentProcessor(\n",
    "    chunk_size=settings.chunk_size,\n",
    "    chunk_overlap=settings.chunk_overlap,\n",
    "    min_chunk_size=settings.min_chunk_size\n",
    ")\n",
    "print(\"✅ Document processor initialized\")\n",
    "\n",
    "# Initialize embedding service\n",
    "print(\"🔄 Initializing embedding service...\")\n",
    "embedding_service = EmbeddingService(\n",
    "    openai_api_key=settings.openai_api_key if hasattr(settings, 'openai_api_key') else None,\n",
    "    model_name=settings.local_embedding_model\n",
    ")\n",
    "print(\"✅ Embedding service initialized\")\n",
    "\n",
    "# Create a ChromaDB-compatible embedding function\n",
    "print(\"🔄 Creating embedding function for ChromaDB...\")\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "# Create a sentence transformers embedding function that ChromaDB can use\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=settings.local_embedding_model\n",
    ")\n",
    "print(\"✅ ChromaDB embedding function created\")\n",
    "\n",
    "# Initialize vector store (connects to backend ChromaDB) WITH embedding function\n",
    "print(\"🔄 Connecting to ChromaDB with embedding function...\")\n",
    "try:\n",
    "    vector_store = VectorStore(\n",
    "        collection_name=settings.collection_name,\n",
    "        persist_directory=settings.vector_db_path,\n",
    "        embedding_function=sentence_transformer_ef  # Add the embedding function!\n",
    "    )\n",
    "    print(\"✅ Connected to ChromaDB successfully with embedding function\")\n",
    "    print(f\"📊 Current document count: {vector_store.count_documents()}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to ChromaDB: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4c615",
   "metadata": {},
   "source": [
    "## 4. Document Discovery and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_documents(documents_dir: str) -> List[Path]:\n",
    "    \"\"\"Discover all supported document files in the directory.\"\"\"\n",
    "    supported_extensions = {'.pdf', '.docx', '.doc', '.txt', '.md', '.markdown'}\n",
    "    documents = []\n",
    "    \n",
    "    documents_path = Path(documents_dir)\n",
    "    \n",
    "    for ext in supported_extensions:\n",
    "        pattern = f\"*{ext}\"\n",
    "        files = list(documents_path.glob(pattern))\n",
    "        documents.extend(files)\n",
    "        if files:\n",
    "            print(f\"📄 Found {len(files)} {ext} files\")\n",
    "    \n",
    "    return sorted(documents)\n",
    "\n",
    "# Discover documents in the documents folder\n",
    "print(\"🔍 Discovering documents...\")\n",
    "document_files = discover_documents(settings.documents_dir)\n",
    "\n",
    "if not document_files:\n",
    "    print(f\"⚠️ No documents found in {settings.documents_dir}\")\n",
    "    print(\"📝 Supported formats: PDF, DOCX, DOC, TXT, MD\")\n",
    "    print(\"💡 Please add some documents to the documents folder and re-run this cell\")\n",
    "else:\n",
    "    print(f\"✅ Found {len(document_files)} documents to process:\")\n",
    "    for doc in document_files:\n",
    "        size_mb = doc.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  📄 {doc.name} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850999c8",
   "metadata": {},
   "source": [
    "## 5. Document Processing and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2167b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "import time\n",
    "\n",
    "def process_document_with_metadata(file_path: Path) -> List[DocumentChunk]:\n",
    "    \"\"\"Process a document file and extract metadata from filename.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n🔄 Processing: {file_path.name}\")\n",
    "        print(f\"📦 File size: {file_path.stat().st_size / (1024 * 1024):.1f} MB\")\n",
    "        \n",
    "        # Extract metadata from filename\n",
    "        print(\"📋 Extracting metadata from filename...\")\n",
    "        filename_metadata = doc_processor.extract_metadata_from_filename(file_path.name)\n",
    "        print(f\" Filename metadata: {filename_metadata}\")\n",
    "\n",
    "        # Add source metadata\n",
    "        source_metadata = {\n",
    "            'processed_at': datetime.now().isoformat(),\n",
    "            'file_path': str(file_path),\n",
    "            **filename_metadata\n",
    "        }\n",
    "        \n",
    "        # OPTIMIZED: Extract text only once and reuse it\n",
    "        print(f\"📖 Reading document: {file_path.name}...\")\n",
    "        extraction_start = time.time()\n",
    "        \n",
    "        # Add timeout protection for text extraction\n",
    "        try:\n",
    "            full_text = doc_processor._extract_text(file_path)\n",
    "            extraction_time = time.time() - extraction_start\n",
    "            print(f\"⏱️ Text extraction took: {extraction_time:.1f} seconds\")\n",
    "        except Exception as extract_error:\n",
    "            print(f\"❌ Text extraction failed: {extract_error}\")\n",
    "            return []\n",
    "        \n",
    "        if not full_text or len(full_text.strip()) == 0:\n",
    "            print(f\"⚠️ WARNING: No text extracted from {file_path.name}\")\n",
    "            return []\n",
    "        \n",
    "        # Calculate comprehensive document statistics\n",
    "        total_chars = len(full_text)\n",
    "        total_words = len(full_text.split())\n",
    "        total_lines = full_text.count('\\n') + 1\n",
    "        estimated_pages = total_words / 250  # Standard: ~250 words per page\n",
    "\n",
    "        print(f\"📊 Document analysis: {len(full_text)/1024:.1f} KB of text, {total_words:,} words\")\n",
    "        \n",
    "        # Add comprehensive document statistics to metadata\n",
    "        source_metadata.update({\n",
    "            'original_char_count': total_chars,\n",
    "            'original_word_count': total_words,\n",
    "            'original_line_count': total_lines,\n",
    "            'estimated_pages': round(estimated_pages, 1),\n",
    "            'text_size_kb': round(len(full_text)/1024, 2)\n",
    "        })\n",
    "        \n",
    "        # OPTIMIZED: Process the text directly instead of re-reading the file\n",
    "        print(f\"✂️ Chunking document...\")\n",
    "        chunking_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            chunks = doc_processor.process_text(full_text, source_metadata)\n",
    "            chunking_time = time.time() - chunking_start\n",
    "            print(f\"⏱️ Chunking took: {chunking_time:.1f} seconds\")\n",
    "        except Exception as chunk_error:\n",
    "            print(f\"❌ Chunking failed: {chunk_error}\")\n",
    "            return []\n",
    "        \n",
    "        if chunks:\n",
    "            # Calculate detailed chunking statistics\n",
    "            chunk_chars = sum(len(chunk.content) for chunk in chunks)\n",
    "            chunk_words = sum(len(chunk.content.split()) for chunk in chunks)\n",
    "            avg_chunk_size = chunk_chars / len(chunks) if chunks else 0\n",
    "            min_chunk_size = min(len(chunk.content) for chunk in chunks)\n",
    "            max_chunk_size = max(len(chunk.content) for chunk in chunks)\n",
    "            \n",
    "            print(f\"✅ CHUNKING RESULTS:\")\n",
    "            print(f\"   🔢 Chunks created: {len(chunks)}\")\n",
    "            print(f\"   📏 Average chunk size: {avg_chunk_size:.0f} chars\")\n",
    "            print(f\"   📏 Chunk size range: {min_chunk_size} - {max_chunk_size} chars\")\n",
    "            \n",
    "            # VERIFY METADATA IS ATTACHED\n",
    "            print(f\"🔍 Metadata verification:\")\n",
    "            sample_chunk = chunks[0] if chunks else None\n",
    "            if sample_chunk and sample_chunk.metadata:\n",
    "                print(f\"   📚 Subject: {sample_chunk.metadata.get('subject', 'MISSING')}\")\n",
    "                print(f\"   🎓 Grade: {sample_chunk.metadata.get('grade', 'MISSING')}\")\n",
    "                print(f\"   📄 Filename: {sample_chunk.metadata.get('filename', 'MISSING')}\")\n",
    "                print(f\"   🗂️ Total metadata fields: {len(sample_chunk.metadata)}\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ WARNING: No metadata found in chunks!\")\n",
    "            \n",
    "            # Calculate and show text retention percentage\n",
    "            if full_text and len(full_text) > 0:\n",
    "                char_retention = (chunk_chars / len(full_text)) * 100\n",
    "                word_retention = (chunk_words / total_words) * 100 if total_words > 0 else 0\n",
    "                print(f\"   📈 Text retention: {char_retention:.1f}% chars, {word_retention:.1f}% words\")\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"✅ Successfully processed: {file_path.name} in {total_time:.1f} seconds\")\n",
    "        else:\n",
    "            print(f\"❌ No chunks created for: {file_path.name}\")\n",
    "            print(\"   Check if the document has sufficient readable text content\")\n",
    "            \n",
    "        # Clear the full_text from memory to help with large files\n",
    "        del full_text\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR processing {file_path.name}: {str(e)}\")\n",
    "        print(f\"   Error type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Check if we have documents to process\n",
    "print(\"🔍 Checking available documents...\")\n",
    "if 'document_files' not in locals() or not document_files:\n",
    "    print(\"❌ No document_files variable found. Please run the document discovery cell first.\")\n",
    "else:\n",
    "    print(f\"✅ Found {len(document_files)} documents to process\")\n",
    "\n",
    "# Process documents with better error handling and progress tracking\n",
    "if 'document_files' in locals() and document_files:\n",
    "    print(\"⚙️ Processing ALL documents with optimized performance...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_chunks = []\n",
    "    processing_summary = []\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    # Sort files by size - process smaller files first for quick feedback\n",
    "    sorted_files = sorted(document_files, key=lambda f: f.stat().st_size)\n",
    "    print(f\"📋 Processing order (by size):\")\n",
    "    for i, doc_file in enumerate(sorted_files):\n",
    "        file_size_mb = doc_file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {i+1}. {doc_file.name} ({file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    for i, doc_file in enumerate(sorted_files, 1):\n",
    "        file_size_mb = doc_file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"\\n📄 DOCUMENT {i}/{len(sorted_files)} - {doc_file.name} ({file_size_mb:.1f} MB)\")\n",
    "        print(f\"🕐 Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        doc_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            chunks = process_document_with_metadata(doc_file)\n",
    "            doc_time = time.time() - doc_start\n",
    "            \n",
    "            if chunks:\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "                # VERIFY METADATA PROPAGATION\n",
    "                first_chunk = chunks[0]\n",
    "                print(f\"📋 Metadata check: Subject='{first_chunk.metadata.get('subject')}', Grade='{first_chunk.metadata.get('grade')}', File='{first_chunk.metadata.get('filename')}'\")\n",
    "                \n",
    "                # Store individual document stats for summary\n",
    "                doc_stats = {\n",
    "                    'filename': doc_file.name,\n",
    "                    'file_size_mb': file_size_mb,\n",
    "                    'processing_time': doc_time,\n",
    "                    'original_words': chunks[0].metadata.get('original_word_count', 0),\n",
    "                    'original_chars': chunks[0].metadata.get('original_char_count', 0),\n",
    "                    'original_lines': chunks[0].metadata.get('original_line_count', 0),\n",
    "                    'estimated_pages': chunks[0].metadata.get('estimated_pages', 0),\n",
    "                    'chunks_created': len(chunks),\n",
    "                    'processed_words': sum(len(chunk.content.split()) for chunk in chunks),\n",
    "                    'subject': chunks[0].metadata.get('subject', 'Unknown'),\n",
    "                    'grade': chunks[0].metadata.get('grade', 'Unknown')\n",
    "                }\n",
    "                processing_summary.append(doc_stats)\n",
    "                \n",
    "                print(f\"⏱️ Document completed in {doc_time:.1f}s - Running total: {len(all_chunks):,} chunks\")\n",
    "            else:\n",
    "                print(f\"⚠️ No chunks generated from {doc_file.name}\")\n",
    "                \n",
    "        except Exception as doc_error:\n",
    "            print(f\"❌ Failed to process {doc_file.name}: {doc_error}\")\n",
    "            \n",
    "        print(\"-\" * 60)  # Separator between documents\n",
    "        \n",
    "        # Safety break - if any single document takes more than 5 minutes, something is wrong\n",
    "        if doc_time > 300:  # 5 minutes\n",
    "            print(f\"⚠️ WARNING: Document processing took {doc_time:.1f}s (>5min). This may indicate an issue.\")\n",
    "    \n",
    "    total_time = time.time() - overall_start\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"📊 DOCUMENT STATISTICS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Show detailed individual document statistics\n",
    "    if processing_summary:\n",
    "        print(f\"{'Document':<35} {'Size(MB)':<8} {'Time(s)':<7} {'Chunks':<7} {'Subject':<12}\")\n",
    "        print(\"-\" * 85)\n",
    "        \n",
    "        for stats in processing_summary:\n",
    "            filename_short = stats['filename'][:32] + \"...\" if len(stats['filename']) > 35 else stats['filename']\n",
    "            \n",
    "            print(f\"{filename_short:<35} \"\n",
    "                  f\"{stats['file_size_mb']:<8.1f} \"\n",
    "                  f\"{stats['processing_time']:<7.1f} \"\n",
    "                  f\"{stats['chunks_created']:<7} \"\n",
    "                  f\"{stats['subject']:<12}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"📊 COMPLETE PROCESSING SUMMARY:\")\n",
    "    print(f\"   📚 Documents processed: {len(document_files)}\")\n",
    "    print(f\"   🔢 Total chunks created: {len(all_chunks):,}\")\n",
    "    print(f\"   ⏱️ Total processing time: {total_time:.1f} seconds\")\n",
    "    if len(all_chunks) > 0 and total_time > 0:\n",
    "        print(f\"   📈 Average speed: {len(all_chunks)/total_time:.1f} chunks/second\")\n",
    "    \n",
    "    if processing_summary:\n",
    "        total_words = sum(s['original_words'] for s in processing_summary)\n",
    "        total_pages = sum(s['estimated_pages'] for s in processing_summary)\n",
    "        unique_subjects = set(s['subject'] for s in processing_summary)\n",
    "        unique_grades = set(s['grade'] for s in processing_summary)\n",
    "        \n",
    "        print(f\"   🔤 Total words across all docs: {total_words:,}\")\n",
    "        print(f\"   📖 Total estimated pages: {total_pages:.1f}\")\n",
    "        print(f\"   📚 Unique subjects: {len(unique_subjects)} - {sorted(unique_subjects)}\")\n",
    "        print(f\"   🎓 Unique grades: {len(unique_grades)} - {sorted(unique_grades)}\")\n",
    "    \n",
    "    # FINAL METADATA VERIFICATION\n",
    "    if all_chunks:\n",
    "        print(f\"\\n🔍 FINAL METADATA VERIFICATION:\")\n",
    "        subjects_found = set()\n",
    "        grades_found = set()\n",
    "        files_found = set()\n",
    "\n",
    "        for chunk in all_chunks:  # Check all chunks\n",
    "            if chunk.metadata:\n",
    "                if chunk.metadata.get('subject'):\n",
    "                    subjects_found.add(chunk.metadata.get('subject'))\n",
    "                if chunk.metadata.get('grade'):\n",
    "                    grades_found.add(chunk.metadata.get('grade'))\n",
    "                if chunk.metadata.get('filename'):\n",
    "                    files_found.add(chunk.metadata.get('filename'))\n",
    "        \n",
    "        print(f\"   📚 Subjects in chunks: {sorted(subjects_found)}\")\n",
    "        print(f\"   🎓 Grades in chunks: {sorted(grades_found)}\")\n",
    "        print(f\"   📄 Files in chunks: {len(files_found)} files\")\n",
    "        \n",
    "        if not subjects_found:\n",
    "            print(f\"   ⚠️ WARNING: No subjects found in chunk metadata!\")\n",
    "        if not grades_found:\n",
    "            print(f\"   ⚠️ WARNING: No grades found in chunk metadata!\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n🎯 Ready to generate embeddings for {len(all_chunks):,} chunks from ALL documents...\")\n",
    "\n",
    "else:\n",
    "    print(\"⏭️ No documents found to process\")\n",
    "    print(\"💡 Place PDF, DOCX, TXT, or MD files in the './documents/' folder\")\n",
    "    \n",
    "    # Set empty variables\n",
    "    all_chunks = []\n",
    "    processing_summary = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebf10b",
   "metadata": {},
   "source": [
    "## 6. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_embeddings_for_chunks(chunks: List[DocumentChunk]) -> List[np.ndarray]:\n",
    "    \"\"\"Generate embeddings for all document chunks.\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"⚠️ No chunks provided for embedding generation\")\n",
    "        return []\n",
    "    \n",
    "    # Extract text content from chunks\n",
    "    texts = [chunk.content for chunk in chunks]\n",
    "    \n",
    "    print(f\"🧠 Generating embeddings for {len(texts)} chunks...\")\n",
    "    print(f\"📊 Estimated processing time: ~{len(texts) * 0.05:.1f} seconds\")\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    use_openai = hasattr(settings, 'use_openai_embeddings') and settings.use_openai_embeddings\n",
    "    \n",
    "    try:\n",
    "        embeddings = await embedding_service.embed_documents(\n",
    "            texts, \n",
    "            use_openai=use_openai, \n",
    "            batch_size=settings.batch_size\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Successfully generated {len(embeddings)} embeddings\")\n",
    "        \n",
    "        if embeddings:\n",
    "            embedding_dim = len(embeddings[0]) if embeddings else 0\n",
    "            print(f\"📏 Embedding dimension: {embedding_dim}\")\n",
    "            print(f\"💾 Memory usage: ~{len(embeddings) * embedding_dim * 4 / 1024 / 1024:.1f} MB\")\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating embeddings: {e}\")\n",
    "        return []\n",
    "\n",
    "# Generate embeddings for ALL processed chunks\n",
    "if 'all_chunks' in locals() and all_chunks:\n",
    "    print(\"🔄 Starting embedding generation for ALL documents...\")\n",
    "    embeddings = await generate_embeddings_for_chunks(all_chunks)\n",
    "    \n",
    "    if embeddings and len(embeddings) == len(all_chunks):\n",
    "        print(f\"✅ Embedding generation successful!\")\n",
    "        print(f\"📊 Ready to upload {len(embeddings)} embeddings to ChromaDB\")\n",
    "    elif embeddings:\n",
    "        print(f\"⚠️ Partial success: {len(embeddings)} embeddings for {len(all_chunks)} chunks\")\n",
    "    else:\n",
    "        print(\"❌ No embeddings generated\")\n",
    "        \n",
    "else:\n",
    "    print(\"⏭️ Skipping embedding generation - no chunks available\")\n",
    "    print(\"💡 Make sure to run the document processing cell first\")\n",
    "    embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f41f6",
   "metadata": {},
   "source": [
    "## 7. Batch Upload to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_chromadb(chunks: List[DocumentChunk], embeddings: List[np.ndarray]):\n",
    "    \"\"\"Upload chunks and embeddings to ChromaDB.\"\"\"\n",
    "    if not chunks or not embeddings:\n",
    "        print(\"⚠️ No data to upload\")\n",
    "        return False\n",
    "    \n",
    "    if len(chunks) != len(embeddings):\n",
    "        print(f\"❌ Mismatch: {len(chunks)} chunks vs {len(embeddings)} embeddings\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"📤 Uploading {len(chunks)} chunks to ChromaDB...\")\n",
    "    print(f\"🎯 Target database: {settings.vector_db_path}\")\n",
    "    print(f\"📚 Collection: {settings.collection_name}\")\n",
    "    \n",
    "    # Show initial database state\n",
    "    initial_count = vector_store.count_documents()\n",
    "    print(f\"📊 Initial document count: {initial_count}\")\n",
    "    \n",
    "    # VERIFY METADATA BEFORE UPLOAD\n",
    "    print(f\"\\n🔍 PRE-UPLOAD METADATA VERIFICATION:\")\n",
    "    subjects_to_upload = set()\n",
    "    grades_to_upload = set()\n",
    "    files_to_upload = set()\n",
    "    \n",
    "    for chunk in chunks:  # Check all chunks\n",
    "        if chunk.metadata:\n",
    "            if chunk.metadata.get('subject'):\n",
    "                subjects_to_upload.add(chunk.metadata.get('subject'))\n",
    "            if chunk.metadata.get('grade'):\n",
    "                grades_to_upload.add(chunk.metadata.get('grade'))\n",
    "            if chunk.metadata.get('filename'):\n",
    "                files_to_upload.add(chunk.metadata.get('filename'))\n",
    "    \n",
    "    print(f\"   📚 Subjects to upload: {sorted(subjects_to_upload)}\")\n",
    "    print(f\"   🎓 Grades to upload: {sorted(grades_to_upload)}\")\n",
    "    print(f\"   📄 Files to upload: {len(files_to_upload)} files\")\n",
    "    \n",
    "    if not subjects_to_upload:\n",
    "        print(f\"   ⚠️ WARNING: No subjects found in chunk metadata before upload!\")\n",
    "        print(f\"   🔍 Sample chunk metadata: {chunks[0].metadata if chunks[0].metadata else 'EMPTY'}\")\n",
    "    \n",
    "    # Prepare data for ChromaDB\n",
    "    documents = [chunk.content for chunk in chunks]\n",
    "    metadatas = [chunk.metadata for chunk in chunks]\n",
    "    ids = [chunk.chunk_id for chunk in chunks]\n",
    "    \n",
    "    # VERIFY METADATA STRUCTURE\n",
    "    print(f\"\\n🔍 METADATA STRUCTURE VERIFICATION:\")\n",
    "    sample_metadata = metadatas[0] if metadatas else {}\n",
    "    print(f\"   📋 Sample metadata keys: {list(sample_metadata.keys()) if sample_metadata else 'NO KEYS'}\")\n",
    "    print(f\"   📚 Sample subject: {sample_metadata.get('subject', 'MISSING') if sample_metadata else 'NO METADATA'}\")\n",
    "    print(f\"   🎓 Sample grade: {sample_metadata.get('grade', 'MISSING') if sample_metadata else 'NO METADATA'}\")\n",
    "    print(f\"   📄 Sample filename: {sample_metadata.get('filename', 'MISSING') if sample_metadata else 'NO METADATA'}\")\n",
    "    \n",
    "    # Upload in batches with progress tracking\n",
    "    batch_size = settings.batch_size\n",
    "    total_uploaded = 0\n",
    "    failed_batches = 0\n",
    "    \n",
    "    print(f\"\\n🔄 Uploading in batches of {batch_size}...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Uploading batches\"):\n",
    "        end_idx = min(i + batch_size, len(chunks))\n",
    "        \n",
    "        batch_documents = documents[i:end_idx]\n",
    "        batch_metadatas = metadatas[i:end_idx]\n",
    "        batch_embeddings = embeddings[i:end_idx]\n",
    "        batch_ids = ids[i:end_idx]\n",
    "        \n",
    "        try:\n",
    "            vector_store.add_documents(\n",
    "                documents=batch_documents,\n",
    "                metadatas=batch_metadatas,\n",
    "                embeddings=batch_embeddings,\n",
    "                ids=batch_ids\n",
    "            )\n",
    "            total_uploaded += len(batch_documents)\n",
    "            \n",
    "            # Progress update every 10 batches\n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"  📈 Progress: {total_uploaded}/{len(chunks)} chunks uploaded\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error uploading batch {i//batch_size + 1}: {e}\")\n",
    "            failed_batches += 1\n",
    "            continue\n",
    "    \n",
    "    # Final results\n",
    "    final_count = vector_store.count_documents()\n",
    "    \n",
    "    print(f\"\\n✅ Upload complete!\")\n",
    "    print(f\"📊 Results:\")\n",
    "    print(f\"  • Successfully uploaded: {total_uploaded} chunks\")\n",
    "    print(f\"  • Failed batches: {failed_batches}\")\n",
    "    print(f\"  • Database before: {initial_count} documents\")\n",
    "    print(f\"  • Database after: {final_count} documents\")\n",
    "    print(f\"  • Net increase: {final_count - initial_count} documents\")\n",
    "    \n",
    "    # POST-UPLOAD VERIFICATION - Sample more strategically\n",
    "    print(f\"\\n🔍 POST-UPLOAD METADATA VERIFICATION:\")\n",
    "    try:\n",
    "        # Get a larger sample to ensure we get documents from all subjects\n",
    "        sample_docs = vector_store.get_documents_by_metadata({}, limit=100)\n",
    "        \n",
    "        if sample_docs and sample_docs.get('metadatas'):\n",
    "            uploaded_subjects = set()\n",
    "            uploaded_grades = set()\n",
    "            uploaded_files = set()\n",
    "            \n",
    "            print(f\"   🔍 Checking {len(sample_docs.get('metadatas', []))} documents for metadata diversity...\")\n",
    "            \n",
    "            for metadata in sample_docs['metadatas']:\n",
    "                if metadata:\n",
    "                    if metadata.get('subject'):\n",
    "                        uploaded_subjects.add(metadata.get('subject'))\n",
    "                    if metadata.get('grade'):\n",
    "                        uploaded_grades.add(metadata.get('grade'))\n",
    "                    if metadata.get('filename'):\n",
    "                        uploaded_files.add(metadata.get('filename'))\n",
    "            \n",
    "            print(f\"   📚 Subjects in database: {sorted(uploaded_subjects)}\")\n",
    "            print(f\"   🎓 Grades in database: {sorted(uploaded_grades)}\")\n",
    "            print(f\"   📄 Files in database: {len(uploaded_files)} files\")\n",
    "            \n",
    "            # Also check with specific subject queries to be sure\n",
    "            print(f\"   📋 Verifying each expected subject exists:\")\n",
    "            for expected_subject in sorted(subjects_to_upload):\n",
    "                subject_docs = vector_store.get_documents_by_metadata({'subject': expected_subject}, limit=1)\n",
    "                if subject_docs and subject_docs.get('metadatas') and len(subject_docs['metadatas']) > 0:\n",
    "                    print(f\"      ✅ {expected_subject}: Found\")\n",
    "                else:\n",
    "                    print(f\"      ❌ {expected_subject}: Missing\")\n",
    "            \n",
    "            if uploaded_subjects == subjects_to_upload:\n",
    "                print(f\"   ✅ Subject metadata preserved correctly!\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ Subject metadata sampling issue detected!\")\n",
    "                print(f\"      Expected: {sorted(subjects_to_upload)}\")\n",
    "                print(f\"      Found in sample: {sorted(uploaded_subjects)}\")\n",
    "                print(f\"   💡 This may be due to document clustering - checking individual subjects above\")\n",
    "        else:\n",
    "            print(f\"   ❌ Could not retrieve documents for verification\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error during post-upload verification: {e}\")\n",
    "    \n",
    "    return total_uploaded > 0\n",
    "\n",
    "# Upload ALL processed data to ChromaDB\n",
    "if 'all_chunks' in locals() and 'embeddings' in locals() and all_chunks and embeddings:\n",
    "    print(\"🚀 Starting complete upload to ChromaDB...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    success = upload_to_chromadb(all_chunks, embeddings)\n",
    "    \n",
    "    if success:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"🎉 UPLOAD SUCCESSFUL!\")\n",
    "        print(\"✅ All documents have been processed and uploaded to ChromaDB\")\n",
    "        print(\"🎯 Your knowledge base is now ready for use\")\n",
    "        print(\"💡 You can now test the search functionality or use the backend API\")\n",
    "    else:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"❌ Upload failed or incomplete\")\n",
    "        print(\"💡 Check the error messages above and try again\")\n",
    "        \n",
    "else:\n",
    "    print(\"⏭️ Skipping upload - missing chunks or embeddings\")\n",
    "    print(\"💡 Make sure to run both the processing and embedding generation cells first\")\n",
    "    \n",
    "    if 'all_chunks' not in locals() or not all_chunks:\n",
    "        print(\"  ❌ No chunks available (run document processing cell)\")\n",
    "    if 'embeddings' not in locals() or not embeddings:\n",
    "        print(\"  ❌ No embeddings available (run embedding generation cell)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b33800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification of all subjects in the database\n",
    "print(\"🔍 MANUAL SUBJECT VERIFICATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "expected_subjects = ['foldrajz', 'irodalom', 'matematika', 'nyelvtan', 'tortenelem']\n",
    "\n",
    "for subject in expected_subjects:\n",
    "    try:\n",
    "        # Check if documents with this subject exist\n",
    "        subject_docs = vector_store.get_documents_by_metadata({'subject': subject}, limit=5)\n",
    "        if subject_docs and subject_docs.get('metadatas') and len(subject_docs['metadatas']) > 0:\n",
    "            count = len(subject_docs['metadatas'])\n",
    "            sample_filename = subject_docs['metadatas'][0].get('filename', 'Unknown')\n",
    "            print(f\"✅ {subject}: Found {count} documents (sample file: {sample_filename})\")\n",
    "        else:\n",
    "            print(f\"❌ {subject}: No documents found\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {subject}: Error checking - {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Also check total counts by getting all documents and counting subjects\n",
    "print(\"📊 COMPREHENSIVE DATABASE ANALYSIS:\")\n",
    "try:\n",
    "    # Get more documents to analyze the full distribution\n",
    "    all_sample_docs = vector_store.collection.get(limit=2843, include=[\"metadatas\"])  # Get all docs\n",
    "    \n",
    "    if all_sample_docs and all_sample_docs.get('metadatas'):\n",
    "        all_subjects = set()\n",
    "        all_files = set()\n",
    "        subject_counts = {}\n",
    "        \n",
    "        for metadata in all_sample_docs['metadatas']:\n",
    "            if metadata and metadata.get('subject'):\n",
    "                subject = metadata.get('subject')\n",
    "                all_subjects.add(subject)\n",
    "                subject_counts[subject] = subject_counts.get(subject, 0) + 1\n",
    "                \n",
    "            if metadata and metadata.get('filename'):\n",
    "                all_files.add(metadata.get('filename'))\n",
    "        \n",
    "        print(f\"📚 All subjects in database: {sorted(all_subjects)}\")\n",
    "        print(f\"📄 All files in database: {len(all_files)} files\")\n",
    "        print(f\"📊 Subject distribution:\")\n",
    "        for subject, count in sorted(subject_counts.items()):\n",
    "            print(f\"   • {subject}: {count:,} chunks\")\n",
    "        \n",
    "        total_chunks_with_subjects = sum(subject_counts.values())\n",
    "        print(f\"📈 Total chunks with subject metadata: {total_chunks_with_subjects:,} / {len(all_sample_docs['metadatas']):,}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Could not retrieve documents for analysis\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during comprehensive analysis: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4835c",
   "metadata": {},
   "source": [
    "## 8. Utility Functions\n",
    "\n",
    "Additional utility functions for managing the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b00c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_knowledge_base():\n",
    "    \"\"\"Reset (clear) the entire knowledge base. Use with caution!\"\"\"\n",
    "    confirm = input(\"⚠️ This will delete ALL documents from ChromaDB. Type 'CONFIRM' to proceed: \")\n",
    "    \n",
    "    if confirm == \"CONFIRM\":\n",
    "        try:\n",
    "            vector_store.reset_collection()\n",
    "            print(\"✅ Knowledge base reset successfully\")\n",
    "            print(f\"📊 New document count: {vector_store.count_documents()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error resetting knowledge base: {e}\")\n",
    "    else:\n",
    "        print(\"❌ Reset cancelled\")\n",
    "\n",
    "def export_processing_report():\n",
    "    \"\"\"Export a processing report with statistics.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"configuration\": {\n",
    "            \"documents_dir\": settings.documents_dir,\n",
    "            \"vector_db_path\": settings.vector_db_path,\n",
    "            \"collection_name\": settings.collection_name,\n",
    "            \"chunk_size\": settings.chunk_size,\n",
    "            \"batch_size\": settings.batch_size\n",
    "        },\n",
    "        \"processing_results\": {\n",
    "            \"documents_found\": len(document_files) if 'document_files' in locals() else 0,\n",
    "            \"chunks_created\": len(all_chunks) if 'all_chunks' in locals() else 0,\n",
    "            \"embeddings_generated\": len(embeddings) if 'embeddings' in locals() else 0,\n",
    "            \"final_db_count\": vector_store.count_documents()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    report_file = f\"processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"📊 Processing report exported to: {report_file}\")\n",
    "    return report\n",
    "\n",
    "# Uncomment the following lines to use these utilities:\n",
    "\n",
    "# Reset knowledge base (CAUTION!)\n",
    "#reset_knowledge_base()\n",
    "\n",
    "# Export processing report\n",
    "report = export_processing_report()\n",
    "print(f\"\\n📋 Processing Summary:\")\n",
    "print(f\"  Documents processed: {report['processing_results']['documents_found']}\")\n",
    "print(f\"  Chunks created: {report['processing_results']['chunks_created']}\")\n",
    "print(f\"  Total documents in DB: {report['processing_results']['final_db_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick diagnostic - let's check what's actually in the database by subject\n",
    "print(\"🔍 Checking database contents by subject...\")\n",
    "\n",
    "# Get a larger sample to see all subjects\n",
    "collection = vector_store.collection\n",
    "sample = collection.get(limit=1000)  # Get more documents\n",
    "\n",
    "if sample and 'metadatas' in sample:\n",
    "    subject_counts = {}\n",
    "    file_counts = {}\n",
    "    \n",
    "    for metadata in sample['metadatas']:\n",
    "        subject = metadata.get('subject', 'unknown')\n",
    "        filename = metadata.get('filename', 'unknown')\n",
    "        \n",
    "        subject_counts[subject] = subject_counts.get(subject, 0) + 1\n",
    "        file_counts[filename] = file_counts.get(filename, 0) + 1\n",
    "    \n",
    "    print(f\"\\n📊 Subject distribution in database:\")\n",
    "    for subject, count in sorted(subject_counts.items()):\n",
    "        print(f\"  📚 {subject}: {count} chunks\")\n",
    "    \n",
    "    print(f\"\\n📄 File distribution in database:\")\n",
    "    for filename, count in sorted(file_counts.items()):\n",
    "        print(f\"  📄 {filename}: {count} chunks\")\n",
    "        \n",
    "    print(f\"\\n📈 Total chunks in database: {sum(subject_counts.values())}\")\n",
    "    \n",
    "    # Test retrieval with OpenAI embeddings\n",
    "    print(f\"\\n🔍 Testing Hungarian query with OpenAI embeddings...\")\n",
    "    try:\n",
    "        # Test with the embedding service using OpenAI\n",
    "        embedding_service = EmbeddingService(\n",
    "            openai_api_key=settings.openai_api_key,\n",
    "            model_name=settings.local_embedding_model\n",
    "        )\n",
    "        \n",
    "        # Test query embedding using OpenAI\n",
    "        test_query = \"mi a matematika?\"\n",
    "        print(f\"🔄 Generating OpenAI embedding for query: '{test_query}'\")\n",
    "        \n",
    "        # Use async embedding with OpenAI\n",
    "        import asyncio\n",
    "        \n",
    "        async def test_openai_embedding():\n",
    "            query_embedding = await embedding_service.embed_text(test_query, use_openai=True)\n",
    "            return query_embedding\n",
    "        \n",
    "        # Run the async function\n",
    "        query_embedding = await test_openai_embedding()\n",
    "        print(f\"✅ OpenAI query embedding generated: shape {query_embedding.shape}\")\n",
    "        \n",
    "        # Search directly in vector store\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=3,\n",
    "            include=['metadatas', 'documents', 'distances']\n",
    "        )\n",
    "        \n",
    "        if results and results['documents']:\n",
    "            print(f\"🔍 Found {len(results['documents'][0])} results for 'mi a matematika?' using OpenAI embeddings:\")\n",
    "            for i, (doc, metadata, distance) in enumerate(zip(\n",
    "                results['documents'][0], \n",
    "                results['metadatas'][0], \n",
    "                results['distances'][0]\n",
    "            )):\n",
    "                subject = metadata.get('subject', 'unknown')\n",
    "                filename = metadata.get('filename', 'unknown')\n",
    "                print(f\"  {i+1}. {subject} ({filename}) - distance: {distance:.3f}\")\n",
    "                print(f\"     📝 {doc[:80]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during OpenAI embedding retrieval test: {e}\")\n",
    "        print(f\"💡 Make sure your OpenAI API key is configured correctly\")\n",
    "        \n",
    "        # Fallback to local embeddings if OpenAI fails\n",
    "        print(f\"\\n🔄 Falling back to local embeddings...\")\n",
    "        try:\n",
    "            query_embedding = embedding_service._embed_with_local_model(test_query)\n",
    "            print(f\"✅ Local query embedding generated: shape {query_embedding.shape}\")\n",
    "            \n",
    "            # Search with local embedding\n",
    "            results = collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=3,\n",
    "                include=['metadatas', 'documents', 'distances']\n",
    "            )\n",
    "            \n",
    "            if results and results['documents']:\n",
    "                print(f\"🔍 Found {len(results['documents'][0])} results using local embeddings:\")\n",
    "                for i, (doc, metadata, distance) in enumerate(zip(\n",
    "                    results['documents'][0], \n",
    "                    results['metadatas'][0], \n",
    "                    results['distances'][0]\n",
    "                )):\n",
    "                    subject = metadata.get('subject', 'unknown')\n",
    "                    filename = metadata.get('filename', 'unknown')\n",
    "                    print(f\"  {i+1}. {subject} ({filename}) - distance: {distance:.3f}\")\n",
    "                    print(f\"     📝 {doc[:80]}...\")\n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Local embedding fallback also failed: {e2}\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ No metadata found in database\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
